# üîß DB SCHEMA & NAMING CONVENTION - Implementation Plan

**Datum:** 2026-01-09  
**Status:** Planning Phase  
**√öƒçel:** Vytvo≈ôit jednotnou DB strukturu pro Phase 5B-6 s jasn√Ωm naming convention

---

## üìä CURRENT STATE

### ‚ùå Probl√©m
- DB tabulky neexistuj√≠ (jen `pg_stat_statements`)
- .env soubor m√° ≈°patn√© credentials (localhost m√≠sto P050TD01)
- Nem√°me definovan√Ω naming convention pro tabulky, sloupce

### ‚úÖ Co m√°me
- DB: `ailog_analyzer` na `P050TD01.DEV.KB.CZ:5432`
- Schema: `ailog_peak` (mƒõlo by existovat)
- User: `ailog_analyzer_user_d1` (data operations)
- DDL User: `ailog_analyzer_ddl_user_d1` (schema operations)

---

## üéØ PLAN - Co se bude dƒõlat

### F√ÅZE 1: DB SETUP & SCHEMA CREATION

#### 1.1 Vytvo≈ôit .env s spr√°vn√Ωmi credentials
- **Soubor:** `.env`
- **Zmƒõny:**
  ```
  DB_HOST=P050TD01.DEV.KB.CZ
  DB_PORT=5432
  DB_NAME=ailog_analyzer
  DB_USER=ailog_analyzer_user_d1
  DB_PASSWORD=<DOPLNIT - z SMAX/emailu>
  DB_DDL_USER=ailog_analyzer_ddl_user_d1
  DB_DDL_PASSWORD=<DOPLNIT - z SMAX/emailu>
  ```
- **Stav:** ‚è≥ PENDING - ƒçek√°m na hesla

#### 1.2 Vytvo≈ôit/Ovƒõ≈ôit schema `ailog_peak`
- **Script:** `scripts/setup_peak_db.py` (upravit/vytvo≈ôit)
- **Operace:**
  ```sql
  CREATE SCHEMA IF NOT EXISTS ailog_peak;
  ```
- **Stav:** ‚è≥ TODO

---

### F√ÅZE 2: TABULKY & NAMING CONVENTION

#### 2.1 Tabulka: `ailog_peak.peak_statistics`

**Prim√°rn√≠ tabulka pro ingestion**

```sql
CREATE TABLE ailog_peak.peak_statistics (
  -- Prim√°rn√≠ identifik√°tory
  id SERIAL PRIMARY KEY,
  
  -- ƒåasov√© informace
  day_of_week INT NOT NULL,           -- 0-6 (Mon-Sun)
  hour_of_day INT NOT NULL,           -- 0-23
  quarter_hour INT NOT NULL,          -- 0-3 (00, 15, 30, 45 minut)
  
  -- Metadata
  namespace VARCHAR(255) NOT NULL,    -- Nap≈ô: pcb-dev-01-app
  
  -- Statistiky
  value FLOAT NOT NULL,               -- Poƒçet error≈Ø v oknƒõ
  
  -- Tracking
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  
  -- Indexy
  UNIQUE (day_of_week, hour_of_day, quarter_hour, namespace)
);

CREATE INDEX idx_ps_namespace ON ailog_peak.peak_statistics(namespace);
CREATE INDEX idx_ps_time_window ON ailog_peak.peak_statistics(day_of_week, hour_of_day, quarter_hour);
```

**Logika:**
- Ukl√°d √Ås norm√°ln√≠ hodnoty (bez peaks)
- Peaks se nahrazuj√≠ referenƒçn√≠ hodnotou (replacement_value)
- Chybƒõj√≠c√≠ okna se vypln√≠ mean=0
- Normalizace bƒõhem reference calc: 0 ‚Üí 1

---

#### 2.2 Tabulka: `ailog_peak.peak_investigation`

**Detailn√≠ anal√Ωza detekovan√Ωch peaks**

```sql
CREATE TABLE ailog_peak.peak_investigation (
  -- Prim√°rn√≠ identifik√°tory
  id SERIAL PRIMARY KEY,
  
  -- ƒåasov√© informace (kdy se peak stal?)
  day_of_week INT NOT NULL,
  hour_of_day INT NOT NULL,
  quarter_hour INT NOT NULL,
  
  -- Metadata
  namespace VARCHAR(255) NOT NULL,    -- Kterou app/NS se to t√Ωk√°?
  app_version VARCHAR(100),           -- Verze aplikace v dan√Ω ƒças
  
  -- Peak data
  original_value FLOAT NOT NULL,      -- Origin√°ln√≠ hodnota (peak)
  reference_value FLOAT NOT NULL,     -- Referenƒçn√≠ baseline
  replacement_value FLOAT,            -- ƒå√≠m se nahradil (pokud null, skip)
  ratio FLOAT NOT NULL,               -- original_value / reference_value
  
  -- Investigace context
  context_before JSONB,               -- ¬±15min okna P≈òED (co se dƒõlo)
  context_after JSONB,                -- ¬±15min okna PO (co se dƒõlo)
  
  -- Status & Tags
  peak_type VARCHAR(50),              -- 'recurring', 'anomaly', 'known'
  known_cause VARCHAR(255),           -- Pokud je to zn√°m√Ω peak
  ai_analysis TEXT,                   -- Output z LLM anal√Ωzy
  
  -- Tracking
  created_at TIMESTAMP DEFAULT NOW(),
  resolved_at TIMESTAMP,              -- Kdy se peak vy≈ôe≈°il (pokud v≈Øbec)
  
  -- Indexy
  UNIQUE (day_of_week, hour_of_day, quarter_hour, namespace)
);

CREATE INDEX idx_pi_namespace ON ailog_peak.peak_investigation(namespace);
CREATE INDEX idx_pi_peak_type ON ailog_peak.peak_investigation(peak_type);
CREATE INDEX idx_pi_created_at ON ailog_peak.peak_investigation(created_at DESC);
```

**Logika:**
- Zaznamen√°v√° V≈†ECHNY detekovan√© peaks
- Ukl√°d√° context (co se dƒõlo p≈ôed/po)
- Linked na LLM pro AI anal√Ωzu
- Tracking: je-li peak recurring, anomaly, nebo j√° zn√°m p≈ô√≠ƒçinu?

---

#### 2.3 Tabulka: `ailog_peak.peak_patterns`

**Tracking rekurentn√≠ch peaks (pro self-learning)**

```sql
CREATE TABLE ailog_peak.peak_patterns (
  -- Prim√°rn√≠ identifik√°tory
  id SERIAL PRIMARY KEY,
  pattern_hash VARCHAR(64),           -- MD5(namespace + day_of_week + hour + quarter) 
  
  -- Pattern metadata
  namespace VARCHAR(255) NOT NULL,
  day_of_week INT,                    -- NULL = v≈°echny dny
  hour_of_day INT,                    -- NULL = v≈°echny hodiny
  quarter_hour INT,                   -- NULL = v≈°echny ƒçtvrthodinky
  
  -- Statistics
  occurrence_count INT DEFAULT 1,     -- Kolikr√°t jsme vidƒõli tento peak?
  avg_original_value FLOAT,           -- Pr≈Ømƒõrn√° height peaku
  last_seen TIMESTAMP,
  first_seen TIMESTAMP,
  
  -- AI & Knowledge
  probable_cause VARCHAR(500),        -- Co to asi zp≈Øsobuje?
  confidence FLOAT DEFAULT 0.5,       -- 0.0-1.0 (confidence v p≈ô√≠ƒçinƒõ)
  recommended_action VARCHAR(500),    -- Co dƒõlat?
  
  -- Status
  is_known BOOLEAN DEFAULT FALSE,     -- Jestli v√≠me co to je
  is_resolved BOOLEAN DEFAULT FALSE,  -- Jestli jsme to vy≈ôe≈°ili
  resolution_notes TEXT,
  
  -- Tracking
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  
  UNIQUE (pattern_hash)
);

CREATE INDEX idx_pp_namespace ON ailog_peak.peak_patterns(namespace);
CREATE INDEX idx_pp_is_known ON ailog_peak.peak_patterns(is_known);
CREATE INDEX idx_pp_last_seen ON ailog_peak.peak_patterns(last_seen DESC);
```

**Logika:**
- Agreguje rekurentn√≠ peaks v stejn√Ω ƒças/namespace
- Uƒç√≠ se: jestli je to nov√Ω peak nebo opakuj√≠c√≠ se?
- Tracking: je-li vy≈ôe≈°en? Jak√° byla p≈ô√≠ƒçina?

---

### F√ÅZE 3: INGESTION SCRIPTS (Co se zmƒõn√≠)

#### 3.1 Script: `scripts/ingest_from_log.py`

**NOV√â FUNKCIONALITY:**

```python
def ingest_peak_statistics_with_detection(log_file, statistics):
    """
    1. Parsuj data z logu
    2. Detekuj peaky (4+4 rule)
    3. Nahraƒè peaky referenƒçn√≠ hodnotou
    4. Insert do peak_statistics
    5. Zaznamenej peaky do peak_investigation
    6. Vypl≈à chybƒõj√≠c√≠ okna (mean=0)
    """
    
    # STEP 1: Parse & Load data
    statistics = parse_peak_statistics_from_log(log_file)
    
    # STEP 2: Iterate all values
    for (day, hour, quarter, namespace), stats in statistics.items():
        value = stats['mean']
        
        # STEP 3: Detect peak
        is_peak, ratio, reference, debug_info = detect_and_skip_peaks(
            day, hour, quarter, namespace, value, statistics
        )
        
        if is_peak:
            # 3a: Record to peak_investigation
            insert_peak_investigation(
                day, hour, quarter, namespace,
                original_value=value,
                reference_value=reference,
                ratio=ratio
            )
            
            # 3b: Replace value
            replacement_value = reference
        else:
            replacement_value = value
        
        # STEP 4: Insert to DB (ALWAYS - no gaps!)
        insert_to_peak_statistics(
            day, hour, quarter, namespace,
            value=replacement_value
        )
    
    # STEP 5: Fill missing windows
    fill_missing_windows()
    
    # STEP 6: Verify
    verify_distribution()
```

**Zmƒõny v k√≥du:**
- Nov√° funcke: `insert_peak_investigation()`
- Integrovan√Ω `fill_missing_windows()` - bez separ√°tn√≠ho scriptu
- Logov√°n√≠ v≈°ech peaks

**Stav:** ‚è≥ TODO - napsat/upravit

---

#### 3.2 Script: `scripts/analyze_peaks_with_llm.py`

**NOV√ù - AI anal√Ωza peaks**

```python
def analyze_peaks_with_llm():
    """
    1. P≈ôeƒçti v≈°echny peaks z peak_investigation
    2. Seskupuj po pattern (recurring vs. anomaly)
    3. Pro ka≈æd√Ω pattern: zavolej LLM
    4. Ulo≈æ v√Ωsledky do peak_investigation.ai_analysis
    5. Aktualizuj peak_patterns tabulku
    """
    
    # Load unanalyzed peaks
    peaks = load_unanalyzed_peaks()
    
    for peak in peaks:
        # Get context from surrounding windows
        context = get_peak_context(peak)
        
        # Call LLM
        analysis = call_ollama_api({
            'namespace': peak.namespace,
            'time': f"{peak.day_of_week} {peak.hour_of_day}:{peak.quarter_hour*15:02d}",
            'peak_value': peak.original_value,
            'reference': peak.reference_value,
            'ratio': peak.ratio,
            'context_before': context['before'],
            'context_after': context['after']
        })
        
        # Save analysis
        update_peak_investigation(peak.id, ai_analysis=analysis)
        
        # Update/create pattern
        update_peak_pattern(peak, analysis)
```

**Stav:** ‚è≥ TODO - vytvo≈ôit

---

### F√ÅZE 4: VERIFICATION & VALIDATION

#### 4.1 Script: `scripts/verify_db_integrity.py`

**Kontroluje:**
- V≈°echny NS maj√≠ v≈°echna okna (7√ó96√ó12 = 8,064 ≈ô√°dk≈Ø)
- ≈Ω√°dn√© NULL hodnoty
- Value range: 0-1000 (peaks by mƒõly b√Ωt nahrazeny)
- peak_investigation records jsou in-sync s replacementem

**Stav:** ‚è≥ TODO - vytvo≈ôit

---

## üìã NAMING CONVENTION (Jednotn√Ω styl)

### Tabulky
- `ailog_peak.peak_statistics` - Norm√°ln√≠ data
- `ailog_peak.peak_investigation` - Detaily peaks
- `ailog_peak.peak_patterns` - Agregovan√© patterns

### Sloupce (Gener√°ln√≠ pravidla)
- `*_at` - Timestamps (created_at, updated_at, resolved_at)
- `*_value` - Numerick√© hodnoty (original_value, reference_value)
- `*_count` - Poƒçty (occurrence_count)
- `is_*` - Boolean flags (is_known, is_resolved)
- `*_type` - Kategorie (peak_type)
- Camel-case: `dayOfWeek` ‚Üí ‚ùå, `day_of_week` ‚Üí ‚úÖ

### Scripts
- `ingest_*` - Data ingestion
- `verify_*` - Validace & kontrola
- `analyze_*` - Anal√Ωza dat
- `export_*` - Export/extraction

---

## ‚úÖ IMPLEMENTAƒåN√ç CHECKLIST

- [ ] **1.1** Doplnit .env credentials
- [ ] **1.2** Spustit `scripts/setup_peak_db.py` (create schema)
- [ ] **2.1** Create table: `peak_statistics`
- [ ] **2.2** Create table: `peak_investigation`
- [ ] **2.3** Create table: `peak_patterns`
- [ ] **3.1** Upravit `ingest_from_log.py` - Peak Detection + Investigation
- [ ] **3.2** Nov√Ω script: `analyze_peaks_with_llm.py`
- [ ] **4.1** Nov√Ω script: `verify_db_integrity.py`
- [ ] **5.0** Spustit INIT Phase 1 (1.12-7.12) ingestion
- [ ] **6.0** Spustit INIT Phase 2 (8.12-14.12) ingestion
- [ ] **7.0** Spustit REGULAR Phase (15.12+)
- [ ] **8.0** Update `working_progress.md` + commit

---

## üìù NEXT ACTIONS (Po≈ôad√≠ spou≈°tƒõn√≠)

### DNES (2026-01-09)
1. Doplnit .env s DB credentials
2. Vytvo≈ôit schema + tabulky
3. Napsat detail plan pro ingest_from_log.py zmƒõny

### Z√çT≈òA (2026-01-10)
1. Upravit ingest_from_log.py
2. Testovat na Phase 1 data (1.12-7.12)
3. Verifikovat data v DB

### POZDƒöJI
1. Analyzovat peaks s LLM
2. Implementovat self-learning
3. Deploy to K8s

---

**Stav:** üîÑ IN PLANNING  
**Maintainer:** jvsete  
**Last Updated:** 2026-01-09 10:30 UTC
