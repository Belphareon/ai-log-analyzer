# üöÄ GETTING STARTED - AI Log Analyzer

**Phase 5B: INIT Phase 3 Weeks - Complete Setup and Execution Guide**

**Version:** 2.0 | **Updated:** 2026-01-12 | **Target:** 24,192 rows baseline

---

## üìã What You'll Do

1. ‚úÖ Verify database connection
2. ‚úÖ Setup DB schema (one-time)
3. ‚úÖ Ingest all 14 data files (21 days)
4. ‚úÖ Fill missing windows (complete grid)
5. ‚úÖ Verify success (24,192 rows)
6. ‚úÖ Create backup

**Estimated Time:** 30-45 minutes

---

## ‚ö†Ô∏è Prerequisites

- ‚úÖ Access to database: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
- ‚úÖ `.env` file with DB credentials
- ‚úÖ All 14 peak_fixed_*.txt files in `/tmp/`
- ‚úÖ Python 3.8+
- ‚úÖ psycopg2 and python-dotenv installed

---

## üöÄ STEP-BY-STEP EXECUTION

### 1.2 V√Ωsledek

- ‚úÖ Po schv√°len√≠ obdr≈æ√≠te **credentials** na sv≈Øj mail
- ‚úÖ Poznamenejte si: **username** a **password**
- ‚úÖ Form√°t √∫ƒçtu: `XX_<NAZEV>_ES_READ`

**P≈ô√≠klad emailu s credentials:**
```
Subject: Tech √∫ƒçet vytvo≈ôen - XX_PCB_ES_READ

Username: XX_PCB_ES_READ
Password: ************
Domain: DS

√öƒçet byl vytvo≈ôen a je p≈ôipraven k pou≈æit√≠.
```

---

## STEP 1: Verify Database Connection

```bash
cd /home/jvsete/git/sas/ai-log-analyzer

python3 << 'EOF'
import os, psycopg2
from dotenv import load_dotenv

load_dotenv()
try:
    conn = psycopg2.connect(
        host=os.getenv('DB_HOST'),
        port=int(os.getenv('DB_PORT')),
        database=os.getenv('DB_NAME'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD')
    )
    print("‚úÖ Database connection successful!")
    conn.close()
except Exception as e:
    print(f"‚ùå Connection failed: {e}")
    print("   Check: .env file exists and DB_HOST/DB_PORT/DB_PASSWORD are correct")
EOF
```

---

## STEP 2: Setup Database (One-Time)

```bash
cd /home/jvsete/git/sas/ai-log-analyzer/scripts

# Create schema and tables
echo "Creating schema..."
python3 setup_peak_db.py

# Grant permissions
echo "Granting permissions..."
python3 grant_permissions.py

echo "‚úÖ Database setup complete!"
```

---

## STEP 3: Ingest All 21 Days (Main Work)

```bash
cd /home/jvsete/git/sas/ai-log-analyzer/scripts

echo "üìä Starting INIT Phase ingestion (3 weeks, no peak detection)..."

# Process all 14 files sequentially
for file in /tmp/peak_fixed_2025_12_01.txt \
            /tmp/peak_fixed_2025_12_02_03.txt \
            /tmp/peak_fixed_2025_12_04_05.txt \
            /tmp/peak_fixed_2025_12_06_07.txt \
            /tmp/peak_fixed_2025_12_08_09.txt \
            /tmp/peak_fixed_2025_12_10_11.txt \
            /tmp/peak_fixed_2025_12_12_13.txt \
            /tmp/peak_fixed_2025_12_14_15.txt \
            /tmp/peak_fixed_2025_12_16.txt \
            /tmp/peak_fixed_2025_12_17.txt \
            /tmp/peak_fixed_2025_12_18.txt \
            /tmp/peak_fixed_2025_12_19.txt \
            /tmp/peak_fixed_2025_12_20.txt \
            /tmp/peak_fixed_2025_12_21.txt
do
  echo "Processing: $(basename $file)"
  python3 ingest_from_log_v2.py --init "$file"
  if [ $? -eq 0 ]; then
    echo "  ‚úÖ Success"
  else
    echo "  ‚ö†Ô∏è Warning - check output above"
  fi
done

echo "‚úÖ All files processed!"
```

**What happens:**
- Reads each file's 1,918 patterns (96 windows √ó 12 namespaces + variations)
- NO peak detection (--init flag disables it)
- Aggregates duplicates using weighted average
- Inserts to peak_statistics

---

## STEP 4: Fill Missing Windows

```bash
cd /home/jvsete/git/sas/ai-log-analyzer/scripts

echo "Completing the grid..."
python3 fill_missing_windows.py

echo "‚úÖ Grid completed!"
```

**Result:** All 24,192 combinations present (21 days √ó 96 windows √ó 12 namespaces)

---

## STEP 5: Verify Success

```bash
cd /home/jvsete/git/sas/ai-log-analyzer

python3 << 'EOF'
import os, psycopg2
from dotenv import load_dotenv

load_dotenv()
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    port=int(os.getenv('DB_PORT')),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD')
)
cursor = conn.cursor()

# Checks
cursor.execute("SELECT COUNT(*) FROM peak_statistics;")
total = cursor.fetchone()[0]

cursor.execute("SELECT COUNT(DISTINCT day_of_week) FROM peak_statistics;")
days = cursor.fetchone()[0]

cursor.execute("SELECT COUNT(DISTINCT namespace) FROM peak_statistics;")
namespaces = cursor.fetchone()[0]

cursor.execute("SELECT COUNT(*) FROM peak_statistics WHERE mean_errors = 0.0;")
zeros = cursor.fetchone()[0]

# Results
print(f"üìä INIT PHASE COMPLETE!")
print(f"‚úÖ Rows: {total}/24192 {'‚úÖ' if total == 24192 else '‚ùå'}")
print(f"‚úÖ Days: {days}/7 {'‚úÖ' if days == 7 else '‚ùå'}")
print(f"‚úÖ Namespaces: {namespaces}/12 {'‚úÖ' if namespaces == 12 else '‚ùå'}")
print(f"‚úÖ Zero rows (OK status): {zeros}")
print(f"‚úÖ Error rows: {total - zeros}")

success = (total == 24192 and days == 7 and namespaces == 12)
if success:
    print(f"\nüéâ READY FOR PHASE 6 (Regular Phase with peak detection)!")
else:
    print(f"\n‚ö†Ô∏è INCOMPLETE - Check counts above")

conn.close()
EOF
```

---

## STEP 6: Create Backup

```bash
cd /home/jvsete/git/sas/ai-log-analyzer

python3 << 'EOF'
import os, psycopg2, csv
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    port=int(os.getenv('DB_PORT')),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD')
)
cursor = conn.cursor()

# Export
cursor.execute("""
    SELECT day_of_week, hour_of_day, quarter_hour, namespace,
           mean_errors, stddev_errors, samples_count
    FROM peak_statistics
    ORDER BY day_of_week, hour_of_day, quarter_hour, namespace;
""")

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
filename = f"/tmp/backup_INIT_3WEEKS_{timestamp}.csv"

with open(filename, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(['day_of_week', 'hour_of_day', 'quarter_hour', 'namespace',
                     'mean_errors', 'stddev_errors', 'samples_count'])
    writer.writerows(cursor.fetchall())

print(f"‚úÖ Backup: {filename}")
conn.close()
EOF
```

---

## ‚úÖ Checklist: Ready for Phase 6?

After completing all 6 steps:

- [ ] Step 1: DB connection works
- [ ] Step 2: Schema created
- [ ] Step 3: All 14 files processed
- [ ] Step 4: Grid filled
- [ ] Step 5: Verification shows 24,192 rows + 7 days + 12 namespaces
- [ ] Step 6: Backup created
- [ ] Update [working_progress.md](working_progress.md)

**When all ‚úÖ ‚Üí Ready for Phase 6: REGULAR Phase (Day 22 onwards)**

---

## üìñ Archive (Older Content)

See `_archive_md/` for older documentation on:
- Elasticsearch setup
- Technical accounts
- Part 2 (Full K8s deployment)
- And more...

---

**Version:** 2.0 | **Updated:** 2026-01-12 | **Phase:** 5B (INIT 3 Weeks)  
# =============================================================================

ES_URL=https://elasticsearch-prod.kb.cz:9200
ES_INDEX=cluster-app_sas-relay-*
ES_USER=XX_RELAY_ES_READ
ES_PASSWORD=your_password_here
ES_VERIFY_CERTS=false
```

**To je v≈°e!** Pro lightweight nepot≈ôebujete datab√°zi, API settings, SECRET_KEY, Ollama, Redis, atd.

---

## Krok 5: Prvn√≠ anal√Ωza (Lightweight)

### 5.1 Ovƒõ≈ôen√≠ p≈ôipojen√≠ k Elasticsearch

#### Test 1: Z√°kladn√≠ p≈ôipojen√≠

```bash
# Jednoduch√Ω test pomoc√≠ curl
curl -u "XX_VASE_APP_ES_READ:vase_heslo" \
  -X GET "https://elasticsearch-test.kb.cz:9500/_cat/indices/cluster-app_vase_aplikace-*?v" \
  --insecure
```

**P≈ô√≠klad pro PCB aplikaci:**
```bash
curl -u "XX_PCB_ES_READ:your_password_here" \
  -X GET "https://elasticsearch-test.kb.cz:9500/_cat/indices/cluster-app_pcb-*?v" \
  --insecure
```

**Oƒçek√°van√Ω v√Ωstup:**
```
health status index                                    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   cluster-app_pcb-api-2025.12.16          xY9kL2mPQR-Tg4nV8fA7Bw   5   1   1234567          0      2.5gb          1.2gb
green  open   cluster-app_pcb-worker-2025.12.16       aB3cD4eF5G-H6iJ7kL8mN9   5   1    987654          0      1.8gb          900mb
green  open   cluster-app_pcb-scheduler-2025.12.16    pQ2rS3tU4V-W5xY6zA7bC8   5   1     45678          0      120mb           60mb
```

#### Test 2: Kontrola dat (poƒçet error≈Ø)

```bash
# Poƒçet error≈Ø za posledn√≠ hodinu pro PCB
curl -u "XX_PCB_ES_READ:your_password_here" \
  -X GET "https://elasticsearch-test.kb.cz:9500/cluster-app_pcb-*/_count" \
  -H 'Content-Type: application/json' \
  -d '{
    "query": {
      "bool": {
        "must": [
          {"match": {"log.level": "ERROR"}},
          {"range": {"@timestamp": {"gte": "now-1h"}}}
        ]
      }
    }
  }' \
  --insecure
```

**Oƒçek√°van√Ω v√Ωstup:**
```json
{
  "count": 2543,
  "_shards": {
    "total": 15,
    "successful": 15,
    "skipped": 0,
    "failed": 0
  }
}
```

‚úÖ Pokud vid√≠te poƒçet > 0, m√°te data a m≈Ø≈æete pokraƒçovat!

### 5.2 Prvn√≠ spu≈°tƒõn√≠ anal√Ωzy (READY!)

Lightweight setup **NEPOT≈òEBUJE** datab√°zi ani API! Rovnou spus≈•te anal√Ωzu:

```bash
# Aktivujte venv (pokud nen√≠ aktivn√≠)
source venv/bin/activate

# Spus≈•te anal√Ωzu posledn√≠ hodiny
python scripts/analyze_period.py \
  --from "$(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M:%SZ')" \
  --to "$(date -u '+%Y-%m-%dT%H:%M:%SZ')" \
  --output first_analysis.json
```

**P≈ô√≠klad s konkr√©tn√≠m ƒçasem:**
```bash
# Anal√Ωza dnes r√°no 8:00-10:00 (UTC ƒças!)
python scripts/analyze_period.py \
  --from "2025-12-16T06:00:00Z" \
  --to "2025-12-16T08:00:00Z" \
  --output morning_analysis.json
```

**Co uvid√≠te bƒõhem bƒõhu:**
```
üîç Fetching errors from Elasticsearch...
‚è≥ Progress: 5000/15234 errors (32.8%) | Batch 1/4
‚è≥ Progress: 10000/15234 errors (65.6%) | Batch 2/4  
‚è≥ Progress: 15000/15234 errors (98.5%) | Batch 3/4
‚úÖ Fetched 15234 errors in 12.3s

üìä Extracting root causes...
‚úÖ Found 156 unique traces
‚úÖ Identified 23 root causes

üìù Generating report...
‚úÖ Analysis complete! Saved to: morning_analysis.json

Summary:
  Total Errors: 15234
  Root Causes: 23
  Top Issue: ConnectionTimeout (4521 errors - 29.7%)
```

**To je v≈°e!** ≈Ω√°dn√© datab√°ze, ≈æ√°dn√© migrace, ≈æ√°dn√° komplexn√≠ infrastruktura. üéâ

---

## Krok 6: Pou≈æit√≠ (Lightweight)

### 6.1 Denn√≠ anal√Ωza

```bash
# Anal√Ωza cel√©ho vƒçerej≈°√≠ho dne
python scripts/analyze_period.py \
  --from "2025-12-15T00:00:00Z" \
  --to "2025-12-15T23:59:59Z" \
  --output daily_2025-12-15.json
```

### 6.2 Anal√Ωza konkr√©tn√≠ho ƒçasov√©ho okna

```bash
# Anal√Ωza ≈°piƒçky dnes r√°no 8-10h
python scripts/analyze_period.py \
  --from "2025-12-16T06:00:00Z" \
  --to "2025-12-16T08:00:00Z" \
  --output morning_peak_2025-12-16.json
```

### 6.3 Real-time anal√Ωza (posledn√≠ hodina)

```bash
# Posledn√≠ hodina
python scripts/analyze_period.py \
  --from "$(date -u -d '1 hour ago' '+%Y-%m-%dT%H:%M:%SZ')" \
  --to "$(date -u '+%Y-%m-%dT%H:%M:%SZ')" \
  --output recent_errors.json
```

### 6.4 Prohl√≠≈æen√≠ v√Ωsledk≈Ø

```bash
# JSON v√Ωstup obsahuje:
cat first_analysis.json

# Zobraz√≠ markdown report:
jq -r '.report' first_analysis.json

# Statistiky:
jq '.statistics' first_analysis.json
```

**P≈ô√≠klad kompletn√≠ho JSON v√Ωstupu:**

```json
{
  "metadata": {
    "analysis_type": "Complete Trace-Based Root Cause Analysis",
    "period_start": "2025-12-16T06:00:00Z",
    "period_end": "2025-12-16T08:00:00Z",
    "duration_seconds": 18.7,
    "timestamp": "2025-12-16T10:23:45Z",
    "total_errors_fetched": 15234,
    "unique_traces": 156,
    "root_causes_identified": 23
  },
  "statistics": {
    "trace_id_coverage_percent": 78.5,
    "app_distribution": {
      "pcb-api": 8234,
      "pcb-worker": 3421,
      "pcb-scheduler": 888,
      "pcb-notification": 691
    },
    "namespace_distribution": {
      "prod-pcb": 12543,
      "prod-pcb-batch": 2691
    },
    "top_root_causes": [
      {
        "issue": "ConnectionTimeout to external API",
        "count": 4521,
        "percentage": 29.7,
        "apps": ["pcb-api", "pcb-worker"]
      },
      {
        "issue": "Database deadlock detected",
        "count": 2134,
        "percentage": 14.0,
        "apps": ["pcb-api"]
      },
      {
        "issue": "Redis connection pool exhausted",
        "count": 1876,
        "percentage": 12.3,
        "apps": ["pcb-worker", "pcb-scheduler"]
      }
    ]
  },
  "report": "# AI Log Analysis Report\n\n## Period: 2025-12-16 06:00 - 08:00 UTC\n\n### Summary\n- Total Errors: 15,234\n- Root Causes: 23\n..."
}
```

**Zobrazen√≠ markdown reportu:**
```bash
# Extrahujte a zobrazte markdown report
jq -r '.report' morning_analysis.json

# Nebo ulo≈æte do souboru
jq -r '.report' morning_analysis.json > report.md
cat report.md
```

### 6.5 Automatizace (Cron)

Pro denn√≠ automatick√© anal√Ωzy:

```bash
# Editujte crontab
crontab -e

# P≈ôidejte (dennƒõ ve 2:00 analyzuje p≈ôedchoz√≠ den)
0 2 * * * cd /home/your-user/git/ai-log-analyzer && \
  ./venv/bin/python scripts/analyze_period.py \
  --from "$(date -u -d 'yesterday 00:00' '+\%Y-\%m-\%dT\%H:\%M:\%SZ')" \
  --to "$(date -u -d 'yesterday 23:59' '+\%Y-\%m-\%dT\%H:\%M:\%SZ')" \
  --output "/var/log/ai-analyzer/daily_$(date -d yesterday '+\%Y-\%m-\%d').json" \
  >> /var/log/ai-analyzer/cron.log 2>&1
```

---

## ‚úÖ Lightweight Setup Complete!

**Gratulujeme!** M√°te funguj√≠c√≠ lightweight setup. üéâ

### Co m√°te:
- ‚úÖ CLI anal√Ωzy kdykoliv pot≈ôebujete
- ‚úÖ JSON + Markdown reporty
- ‚úÖ ≈Ω√°dn√° infrastruktura k √∫dr≈æbƒõ
- ‚úÖ Rychl√© a jednoduch√©

### Co NEM√ÅTE (a nepot≈ôebujete pro lightweight):
- ‚ùå REST API
- ‚ùå PostgreSQL datab√°ze
- ‚ùå Self-learning
- ‚ùå Historick√° data
- ‚ùå Redis caching

### üöÄ Chcete v√≠ce? Pokraƒçujte na [Part 2: Full Kubernetes](#part-2-full-kubernetes-deployment)!

---
---

# Part 2: Full (Kubernetes Deployment)

**Production-ready setup s REST API, datab√°z√≠ a automatizac√≠**

---

## üìã Obsah - Part 2

1. [Prerekvizity (Full)](#prerekvizity-full)
2. [Tech √∫ƒçet](#tech-√∫ƒçet-stejn√©-jako-part-1)
3. [Instalace (Full)](#krok-3-instalace-projektu-full)
4. [Konfigurace (Full)](#krok-4-konfigurace-full)
5. [Lok√°ln√≠ testov√°n√≠](#krok-5-lok√°ln√≠-testov√°n√≠-full)
6. [K8s Deployment](#krok-6-kubernetes-deployment)
7. [Monitoring & Alerting](#krok-7-monitoring--alerting)

---

## Prerekvizity (Full)

### Co pot≈ôebujete pro full setup:

**Stejn√© jako Part 1:**
- ‚úÖ **P≈ô√≠stup k SMAX** - pro vytvo≈ôen√≠ technick√©ho √∫ƒçtu
- ‚úÖ **JIRA p≈ô√≠stup** - pro povolen√≠ ES p≈ô√≠stupu
- ‚úÖ **Python 3.11+** - nainstalovan√Ω na lok√°ln√≠m stroji
- ‚úÖ **Git** - pro klonov√°n√≠ reposit√°≈ôe
- ‚úÖ **Elasticsearch cluster** - znalost n√°zvu va≈°eho indexu
- ‚úÖ **Znalost jm√©na va≈°√≠ aplikace** - nap≈ô. `pcb`, `sas-relay`, atd.

**NAV√çC pro full:**
- ‚úÖ **PostgreSQL 16+** - produkƒçn√≠ datab√°ze
- ‚úÖ **Docker & Docker Compose** - pro lok√°ln√≠ v√Ωvoj
- ‚úÖ **Kubernetes cluster** - pro deployment
- ‚úÖ **kubectl** - konfigurovan√Ω p≈ô√≠stup do K8s
- ‚úÖ **Harbor registry** - pro Docker images
- ‚úÖ **CyberArk** - pro ukl√°d√°n√≠ credentials (optional)
- ‚úÖ **Ollama** - pro LLM anal√Ωzu (optional)
- ‚úÖ **Redis** - pro caching (optional)

---

## Tech √∫ƒçet (Stejn√© jako Part 1)

Pokud jste ji≈æ vytvo≈ôili tech √∫ƒçet v **Part 1**, m≈Ø≈æete tento krok p≈ôeskoƒçit.

Jinak postupujte podle **[Krok 1](#krok-1-vytvo≈ôen√≠-technick√©ho-√∫ƒçtu)** a **[Krok 2](#krok-2-povolen√≠-p≈ô√≠stupu-do-elasticsearch)** z Part 1.

---

## Krok 3: Instalace projektu (Full)

### 3.1 Klonov√°n√≠ reposit√°≈ôe

```bash
cd ~/git
git clone <url-repositare> ai-log-analyzer
cd ai-log-analyzer
```

### 3.2 Vytvo≈ôen√≠ virtu√°ln√≠ho prost≈ôed√≠

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# nebo
venv\Scripts\activate  # Windows
```

### 3.3 Instalace V≈†ECH z√°vislost√≠

Pro full setup instalujeme v≈°e:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 3.4 Spu≈°tƒõn√≠ lok√°ln√≠ infrastruktury (Docker)

```bash
# Spust√≠ PostgreSQL + Ollama + Redis
docker-compose up -d
```

Ovƒõ≈ôen√≠:

```bash
docker-compose ps
# Mƒõli byste vidƒõt: postgres, ollama, redis (v≈°echny "Up")
```

---

## Krok 4: Konfigurace (Full)

### 4.1 Vytvo≈ôen√≠ `.env` souboru (KOMPLETN√ç)

```bash
# Zkop√≠rujte template s p≈ô√≠klady
cp .env.example .env

# Upravte v≈°echny hodnoty podle va≈°eho prost≈ôed√≠
nano .env
```

**D≈ÆLE≈ΩIT√â:** Soubor `.env` je v `.gitignore` a NEBUDE nahr√°n do gitu. Va≈°e credentials jsou v bezpeƒç√≠!

### 4.2 Kompletn√≠ konfigurace `.env`

```bash
# =============================================================================
# AI LOG ANALYZER - FULL CONFIGURATION
# =============================================================================
# Production-ready setup s datab√°z√≠, API, LLM
# =============================================================================

# -----------------------------------------------------------------------------
# DATABASE - PostgreSQL
# -----------------------------------------------------------------------------
DATABASE_URL=postgresql://ailog:ailog_dev_pass@localhost:5432/ailog_analyzer

# -----------------------------------------------------------------------------
# ELASTICSEARCH - VA≈†E HODNOTY!
# -----------------------------------------------------------------------------
# URL va≈°eho Elasticsearch clusteru
ES_URL=https://elasticsearch.vase-domena.cz:9200

# N√°zev va≈°eho indexu (pattern)
ES_INDEX=cluster-app_<VASE_APLIKACE>-*

# Technick√Ω √∫ƒçet z SMAX (Krok 1)
ES_USER=XX_<VASE_APLIKACE>_ES_READ
ES_PASSWORD=<heslo_z_emailu>

# SSL/TLS nastaven√≠
ES_VERIFY_CERTS=false

# -----------------------------------------------------------------------------
# API SETTINGS
# -----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# -----------------------------------------------------------------------------
# SECURITY
# -----------------------------------------------------------------------------
# Vygenerujte vlastn√≠ secret key:
# python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=<vygenerovany_secret_key>
ALGORITHM=HS256

# -----------------------------------------------------------------------------
# OLLAMA LLM (Optional - pro AI anal√Ωzu)
# -----------------------------------------------------------------------------
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=mistral:latest

# -----------------------------------------------------------------------------
# REDIS (Optional - pro caching)
# -----------------------------------------------------------------------------
REDIS_URL=redis://localhost:6379

# -----------------------------------------------------------------------------
# LOGGING
# -----------------------------------------------------------------------------
LOG_LEVEL=INFO
LOG_FORMAT=json

# -----------------------------------------------------------------------------
# ANALYSIS SETTINGS
# -----------------------------------------------------------------------------
LEARNING_ENABLED=true
AUTO_ADJUST_THRESHOLDS=true
MIN_SAMPLES_FOR_LEARNING=10
```

### 4.3 Generov√°n√≠ SECRET_KEY

```bash
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

Zkop√≠rujte v√Ωstup do `.env` jako hodnotu `SECRET_KEY`.

---

## Krok 5: Lok√°ln√≠ testov√°n√≠ (Full)

### 5.1 Inicializace datab√°ze

```bash
# Spus≈•te Alembic migrace
alembic upgrade head
```

### 5.2 Test ES p≈ôipojen√≠

```bash
curl -u "$ES_USER:$ES_PASSWORD" "$ES_URL/_cluster/health" --insecure
```

### 5.3 Spu≈°tƒõn√≠ API serveru

```bash
# Development mode
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Otev≈ôete: http://localhost:8000/docs (Swagger UI)

### 5.4 Test API

```bash
# Health check
curl http://localhost:8000/health

# Analyze endpoint
curl -X POST http://localhost:8000/api/v1/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "time_from": "2025-12-16T07:00:00Z",
    "time_to": "2025-12-16T08:00:00Z",
    "app_filter": "<vase-aplikace>-*"
  }'
```

---

## Krok 6: Kubernetes Deployment

### 6.1 P≈ô√≠prava Docker image

```bash
# Build image
docker build -t harbor.vase-domena.cz/ai-log-analyzer:v1.0.0 .

# Push to Harbor
docker push harbor.vase-domena.cz/ai-log-analyzer:v1.0.0
```

### 6.2 Vytvo≈ôen√≠ K8s Secret s credentials

```bash
# ES credentials
kubectl create secret generic ai-log-analyzer-es-creds \
  --from-literal=ES_USER='XX_VASE_APP_ES_READ' \
  --from-literal=ES_PASSWORD='vase_heslo' \
  -n your-namespace

# Database credentials (pro prod PostgreSQL)
kubectl create secret generic ai-log-analyzer-db-creds \
  --from-literal=DATABASE_URL='postgresql://user:pass@postgres-host:5432/ailog_analyzer' \
  -n your-namespace
```

### 6.3 √öprava K8s manifest≈Ø

```bash
# Upravte ConfigMap
vim k8s/configmap.yaml

# Zmƒõ≈àte hodnoty:
# - ES_URL
# - ES_INDEX
# - OLLAMA_URL (pokud m√°te)
```

### 6.4 Deploy do K8s

```bash
# Deploy v≈°echny manifesty
kubectl apply -f k8s/ -n your-namespace

# Ovƒõ≈ôen√≠
kubectl get pods -n your-namespace
kubectl get svc -n your-namespace
```

### 6.5 Ovƒõ≈ôen√≠ deploymentu

```bash
# Logy
kubectl logs -f deployment/ai-log-analyzer -n your-namespace

# Port-forward pro testov√°n√≠
kubectl port-forward svc/ai-log-analyzer 8000:8000 -n your-namespace

# Test API
curl http://localhost:8000/health
```

---

## Krok 7: Monitoring & Alerting

### 7.1 Prometheus metriky

API automaticky exportuje metriky na `/metrics`:

```bash
curl http://localhost:8000/metrics
```

### 7.2 Grafana dashboard

Import dashboard z `k8s/grafana-dashboard.json` (pokud existuje).

### 7.3 Alerty

Nakonfigurujte alerty pro:
- ‚úÖ API response time > 5s
- ‚úÖ Error rate > 1%
- ‚úÖ Database connection errors
- ‚úÖ ES query failures

---

## ‚úÖ Full Setup Complete!

**Gratulujeme!** M√°te plnƒõ funkƒçn√≠ production-ready deployment. üöÄ

### Co m√°te:
- ‚úÖ REST API s Swagger dokumentac√≠
- ‚úÖ PostgreSQL datab√°ze s histori√≠
- ‚úÖ Self-learning z feedback
- ‚úÖ Automatizovan√© denn√≠ anal√Ωzy
- ‚úÖ K8s deployment s HA
- ‚úÖ Monitoring & alerting
- ‚úÖ Redis caching (pokud nakonfigurov√°n)
- ‚úÖ LLM anal√Ωza (pokud Ollama nakonfigurov√°n)

---
---

# Spoleƒçn√© sekce pro obƒõ varianty

---

## üìö Dal≈°√≠ dokumentace

### Pro obƒõ varianty (Lightweight i Full):

- **[HOW_TO_USE.md](HOW_TO_USE.md)** - Detailn√≠ operaƒçn√≠ p≈ô√≠ruƒçka
- **[README.md](README.md)** - P≈ôehled projektu a features
- **[CONTEXT_RETRIEVAL_PROTOCOL.md](CONTEXT_RETRIEVAL_PROTOCOL.md)** - Quick reference
- **[scripts/INDEX.md](scripts/INDEX.md)** - Dokumentace v≈°ech skript≈Ø

---

## üîß Customizace pro va≈°i aplikaci

### Upravte prahy detekce (optional)

```python
# app/services/analysis.py
ERROR_THRESHOLD = 100  # minim√°ln√≠ poƒçet error≈Ø pro alert
SPIKE_MULTIPLIER = 2.5  # kolikr√°t v√≠c ne≈æ baseline = spike
```

### Upravte seznam monitorovan√Ωch aplikac√≠ (optional)

```python
# app/core/config.py
MONITORED_APPS = [
    "vase-aplikace-api",
    "vase-aplikace-worker",
    "vase-aplikace-scheduler"
]
```

---

## üîç Troubleshooting

### Problem: Nelze se p≈ôipojit k Elasticsearch

**≈òe≈°en√≠:**

```bash
# 1. Ovƒõ≈ôte credentials
echo $ES_USER
echo $ES_PASSWORD

# 2. Test p≈ôipojen√≠
curl -u "$ES_USER:$ES_PASSWORD" "$ES_URL/_cluster/health" --insecure

# 3. Zkontrolujte firewall/VPN
ping elasticsearch-test.kb.cz
```

### Problem: "Permission denied" na indexech

**≈òe≈°en√≠:**

- ‚úÖ Ovƒõ≈ôte JIRA ticket (Krok 2) - je schv√°len√Ω?
- ‚úÖ Zkontrolujte pattern indexu: `cluster-app_<app>-*`
- ‚úÖ Kontaktujte ES admin team

### Problem: ≈Ω√°dn√° data v anal√Ωze

**≈òe≈°en√≠:**

```bash
# 1. Ovƒõ≈ôte, ≈æe index obsahuje data
curl -u "$ES_USER:$ES_PASSWORD" \
  "$ES_URL/cluster-app_vase-aplikace-*/_count" --insecure

# 2. Zkontrolujte ƒçasov√© rozmez√≠
# ES pou≈æ√≠v√° UTC! P≈ôepoƒç√≠tejte lok√°ln√≠ ƒças na UTC

# 3. Ovƒõ≈ôte filtr v ES query
# Zkontrolujte log.level: ERROR vs error vs Error
```

### Problem: Database connection failed

**≈òe≈°en√≠:**

```bash
# 1. Je PostgreSQL spu≈°tƒõn√Ω?
docker-compose ps postgres
# nebo
systemctl status postgresql

# 2. Ovƒõ≈ôte DATABASE_URL v .env
echo $DATABASE_URL

# 3. Test p≈ôipojen√≠
psql "$DATABASE_URL"
```

### Problem: Ollama LLM nedostupn√Ω

**≈òe≈°en√≠:**

```bash
# 1. Spus≈•te Ollama
docker-compose up -d ollama

# 2. St√°hnƒõte model
docker exec -it ai-log-analyzer-ollama-1 ollama pull mistral

# 3. Nebo pou≈æijte mock mode
# V .env nastavte:
OLLAMA_URL=mock://localhost
```

---

## üìû Podpora

### Dokumentace
- **GitHub Wiki:** [Link na wiki]
- **JIRA:** [Link na JIRA projekt]
- **Confluence:** [Link na confluence]

### Kontakty
- **DevOps Team:** devops@vase-domena.cz
- **ES Admin:** elasticsearch-admin@vase-domena.cz
- **Slack:** #ai-log-analyzer

---

## ‚úÖ Checklist pro go-live

P≈ôed nasazen√≠m do produkce ovƒõ≈ôte:

- [ ] ‚úÖ Tech √∫ƒçet vytvo≈ôen a credentials ulo≈æeny bezpeƒçnƒõ
- [ ] ‚úÖ ES p≈ô√≠stup povolen p≈ôes JIRA
- [ ] ‚úÖ Projekt nainstalov√°n a z√°vislosti nainstalov√°ny
- [ ] ‚úÖ `.env` soubor nakonfigurov√°n s va≈°imi hodnotami
- [ ] ‚úÖ PostgreSQL datab√°ze bƒõ≈æ√≠ a migrace aplikov√°ny
- [ ] ‚úÖ Test p≈ôipojen√≠ k ES √∫spƒõ≈°n√Ω
- [ ] ‚úÖ Prvn√≠ anal√Ωza probƒõhla bez chyb
- [ ] ‚úÖ V√Ωstup anal√Ωzy zkontrolov√°n a validn√≠
- [ ] ‚úÖ Cron job nebo K8s deployment nastaven
- [ ] ‚úÖ Monitoring a alerting nakonfigurov√°n
- [ ] ‚úÖ Dokumentace p≈ôeƒçtena a pochopena

---

**Gratulujeme! üéâ**

V√°≈° AI Log Analyzer je p≈ôipraven k pou≈æit√≠. Zaƒçnƒõte s denn√≠mi anal√Ωzami a nechte AI pomoci objevovat patterns ve va≈°ich log√°ch.

**Happy analyzing! üöÄ**
