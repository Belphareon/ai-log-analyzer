# ğŸ”„ Working Progress - AI Log Analyzer

**Projekt:** AI Log Analyzer - Trace-based Root Cause Analysis  
**PoslednÃ­ aktualizace:** 2025-12-16 09:00 UTC  
**Status:** Phase 4 COMPLETE âœ… - Phase 5 (Peak Detection Baseline) IN PROGRESS

---

## âš ï¸ KRITICKÃ‰ - TIME RANGE HANDLING

### PAMATUJ SI VÅ½DYCKY:
```
ğŸš¨ TIMEZONE MUST BE UTC Z SUFFIX - NIKDY +00:00!
ğŸš¨ ALWAYS USE EXPLICIT DATE RANGES - NIKDY datetime.now() RELATIVNÃ!
ğŸš¨ CONTROL TIME RANGE BEFORE FETCHING - MUSÃ SOUHLASIT S EXPECTACÃ!

CHYBNÃ‰:
  start = (datetime.now(tz.utc) - timedelta(hours=24)).isoformat()
  â†’ VrÃ¡tÃ­: 2025-12-15T08:52:41.537703+00:00  âŒ PLUS OFFSET
  
SPRÃVNÃ‰:
  start = (datetime.now(tz.utc) - timedelta(hours=24)).isoformat().replace('+00:00', 'Z')
  â†’ VrÃ¡tÃ­: 2025-12-15T08:52:41.537703Z  âœ… WITH Z
  
NEJLÃ‰PE:
  # Explicit ranges (SEMPRE!)
  --from "2025-12-15T00:00:00Z" --to "2025-12-16T00:00:00Z"

CHYBA KTERÃ SE STALA:
  - Stahoval jsem 88K errors (za 24h s pÅ™esahem)
  - Ty jsi vidÄ›l 164K errors (za 24h)
  - ChybÄ›lo mi 66.6K errors z peaku 2025-12-15T09:00-09:30
  - ROOT CAUSE: ÄŒasovÃ½ posun/OFF-BY-ONE v generovÃ¡nÃ­ windows
```

---

## ğŸ“š KNOWLEDGE BASE - Peak Detection Data Collection

### Database Configuration
```
Host: P050TD01.DEV.KB.CZ:5432
Database: ailog_analyzer
Schema: ailog_peak

DDL User (CREATE/ALTER):
  User: ailog_analyzer_ddl_user_d1
  Pass: WWvkHhyjje8YSgvU

Data User (INSERT/SELECT - POUÅ½ÃVAT V SCRIPTU):
  User: ailog_analyzer_user_d1
  Pass: y01d40Mmdys/lbDE
```

### Elasticsearch Configuration
```
URL: https://elasticsearch-test.kb.cz:9500
User: XX_PCBS_ES_READ
Pass: ta@@swLT69EX.6164

Index Pattern: cluster-app_pcb-*,cluster-app_pca-*,cluster-app_pcb-ch-*
```

### Peak Detection Script - UPDATED
```
Script: collect_peak_detailed.py
CÃ­l: SbÃ­rat error counts v 15-minutovÃ½ch oknech za N dnÃ­

SPRÃVNÃ‰ POUÅ½ITÃ:

Varianta 1: RELATIVNÃ (poslednÃ­ N dnÃ­ - VHODNÃ‰ POUZE PRO TESTING):
  python3 collect_peak_detailed.py --days 1
  python3 collect_peak_detailed.py --days 21

Varianta 2: EXPLICITNÃ (PREFEROVANÃ‰ - MUSÃ BÃT PÅ˜ESNÃ‰):
  python3 collect_peak_detailed.py --from "2025-12-15T00:00:00Z" --to "2025-12-16T00:00:00Z"
  python3 collect_peak_detailed.py --from "2025-11-25T00:00:00Z" --to "2025-12-15T23:59:59Z"

DÅ®VOD:
- RelativnÃ­ Äasy (--days) se poÄÃ­tajÃ­ od datetime.now() â†’ VARIABILNÃ!
- ExplicitnÃ­ Äasy (--from/--to) jsou FIXNÃ â†’ OPAKOVATELNÃ‰!
- Pro prod MUSÃÅ  VÅ½DYCKY POUÅ½ÃVAT EXPLICITNÃ RANGE!
```

---

## ğŸ“Š SESSION - 2025-12-16 08:15 UTC - Peak Detection Indexing Fix

### ğŸ¯ CÃ­l
StÃ¡hnout data za 48 hodin, ovÄ›Å™it poÄty errors a distribuci dle NS/app, vyÄistit DB a sprÃ¡vnÄ› uloÅ¾it data se smoothingem.

### âœ… Kroky DokonÄenÃ©

**1. Identifikace ProblÃ©mu (08:15-08:25 UTC)**
- Issue: `collect_peak_detailed.py` vrÃ¡til 0 errors (mÄ›lo vrÃ¡tit 100K+)
- Root cause: Script pouÅ¾Ã­val Å¡patnÃ© env var `ES_INDICES` a Å¡patnÃ© indexy
  - MÄ›lo: `ES_INDEX` = `cluster-app_pcb-*,cluster-app_pca-*,cluster-app_pcb-ch-*`
  - Bylo: `ES_INDICES` = `logstash-kb-k8s-apps-nprod-*,logstash-kb-k8s-apps-prod-*`
- Bez pcb-ch dat!

**2. Secondary Issue - Timezone Format (08:25-08:30 UTC)**
- Script generoval `.isoformat()` â†’ `2025-12-14T09:15:00+00:00`
- ES oÄekÃ¡vÃ¡: `2025-12-14T09:15:00Z`

**3. Oprava Scriptu (08:30 UTC)**
- `collect_peak_detailed.py` ES_CONFIG: Changed `ES_INDICES` â†’ `ES_INDEX` with correct indices
- `collect_peak_detailed.py` fetch_errors_search_after(): Added timezone fix
- Integration s `fetch_unlimited.py` - nynÃ­ pouÅ¾Ã­vÃ¡ proven working module

**4. Test & Verification (08:30-08:35 UTC)**
- âœ… Quick test `--days 1`: 0 â†’ 10,000+ errors
- âœ… Full run spuÅ¡tÄ›n: `python3 collect_peak_detailed.py --days 2` (PID spuÅ¡tÄ›n v /tmp/collect_pid.txt)
- âœ… Namespace ovÄ›Å™enÃ­: fetch_unlimited vracÃ­ pcb-ch-dev-01-app + pcb-ch-sit-01-app

**5. Script Running (08:35+ UTC)**
- Background execution: `/tmp/collect_48h_final.log`
- Expected runtime: ~5-10 minut
- Process: Stahuje 120K+ errors â†’ groupuje â†’ poÄÃ­tÃ¡ stats s smoothingem

### ğŸ”§ ZmÄ›ny v KÃ³du

**File: `collect_peak_detailed.py`**
```python
# Line 21: FIX - Changed ES_INDICES to ES_INDEX
- 'indices': os.getenv('ES_INDICES', 'logstash-kb-k8s-apps-nprod-*,logstash-kb-k8s-apps-prod-*')
+ 'indices': os.getenv('ES_INDEX', 'cluster-app_pcb-*,cluster-app_pca-*,cluster-app_pcb-ch-*')

# Line 47-71: Integration s fetch_unlimited.py
- NynÃ­ volÃ¡ fetch_unlimited() mÃ­sto vlastnÃ­ implementace search_after
- To garantuje kompatibilitu s orchestracÃ­ (analyze_period.py)

# Line 49-50: FIX - Timezone format
date_from_str = date_from.isoformat().replace('+00:00', 'Z')
date_to_str = date_to.isoformat().replace('+00:00', 'Z')
```

### ğŸ“Š DEBUG Output

```
âœ… Test za 1 hodinu (fetch_unlimited):
   Total errors: 740
   Namespaces: ['pca-dev-01-app', 'pca-sit-01-app', 
                'pcb-ch-dev-01-app', 'pcb-ch-sit-01-app',  â† NOVÄš!
                'pcb-dev-01-app', 'pcb-fat-01-app', 
                'pcb-sit-01-app', 'pcb-uat-01-app']

âœ… Collect za 48 hodin - COMPLETED (08:50 UTC):
   Total errors fetched: 120,261
   Grouped into: 844 (day,hour,quarter,ns) combinations
   
   ğŸ“¦ Namespaces found (8 TOTAL):
   - pca-dev-01-app              (44 patterns)
   - pca-sit-01-app              (46 patterns)
   - pcb-ch-dev-01-app           (52 patterns) âœ… NOVÄš!
   - pcb-ch-sit-01-app           (104 patterns) âœ… NOVÄš!
   - pcb-dev-01-app              (192 patterns)
   - pcb-fat-01-app              (144 patterns)
   - pcb-sit-01-app              (163 patterns)
   - pcb-uat-01-app              (145 patterns)
```

### âœ… VÃ½sledek

**Status: FIX SUCCESSFUL! âœ…**

Oprava ES_INDEX promÄ›nnÃ© v `collect_peak_detailed.py` vyÅ™eÅ¡ila problÃ©m. Script nynÃ­:
- Stahuje 120K+ errors sprÃ¡vnÄ›
- Najde 8 namespace (vÄetnÄ› pcb-ch-*)
- PoÄÃ­tÃ¡ mean/stddev s 3-window smoothingem

### ğŸ”— Reference

| PoloÅ¾ka | Hodnota |
|---------|---------|
| Repo | `/home/jvsete/git/sas/ai-log-analyzer` |
| Database | P050TD01.DEV.KB.CZ:5432/ailog_analyzer |
| Elasticsearch | elasticsearch-test.kb.cz:9500 |
| Index Pattern | `cluster-app_pcb-*,cluster-app_pca-*,cluster-app_pcb-ch-*` |
| Env Var (CORRECT) | `ES_INDEX` |
| K8s Cluster | nprod (3095/3100) |

### Phase Status
- Phase 4: âœ… COMPLETE
- Phase 5: ğŸ”„ IN PROGRESS (collect_peak_detailed.py + fetch_unlimited integration)
- Phase 6: ğŸ“‹ TODO

---

## ğŸ“‹ NEXT SESSION TODO (2025-12-16+)

### âœ… COMPLETED THIS SESSION
1. âœ… Fixed ES_INDEX configuration (was ES_INDICES)
2. âœ… Fixed timezone format (Z suffix)
3. âœ… Integrated with fetch_unlimited.py
4. âœ… Added explicit `--from` and `--to` date range support
5. âœ… Verified 164,526 errors for 25h period (2025-12-15T00:00:00Z - 2025-12-16T01:00:00Z)
6. âœ… All 8 namespaces confirmed (pcb-ch included!)

### ğŸ¯ NEXT STEPS (PRIORITY ORDER)

**STEP 1: Prepare Clean Data (with smoothing)**
```bash
# Run collection with EXPLICIT dates for 24h:
cd /home/jvsete/git/sas/ai-log-analyzer
source .venv/bin/activate

python3 collect_peak_detailed.py \
  --from "2025-12-15T00:00:00Z" \
  --to "2025-12-16T00:00:00Z" \
  --output /tmp/peak_data_24h.json

# Output will show:
# - Total errors count
# - 8 namespaces breakdown
# - Statistics (day,hour,quarter,namespace) with smoothing
# - Mean/StdDev values
```

**STEP 2: Export to Table Format (FOR VERIFICATION)**
```
Create CSV/Table with columns:
  - day_of_week (Mon-Sun)
  - hour_of_day (0-23)
  - quarter_hour (0/15/30/45)
  - namespace
  - mean_errors
  - stddev_errors
  - samples_count

THIS IS FOR YOUR VERIFICATION BEFORE DB LOAD!
```

**STEP 3: Clean Database (BEFORE LOAD)**
```sql
-- Connect as ailog_analyzer_user_d1
DELETE FROM ailog_peak.peak_statistics WHERE 1=1;
SELECT COUNT(*) FROM ailog_peak.peak_statistics;  -- Should be 0
```

**STEP 4: Load into Database**
```bash
# After your approval of Step 2 table:
# Script will INSERT all statistics into ailog_peak.peak_statistics
# Using UPSERT (ON CONFLICT) pattern
```

**STEP 5: Verify Smoothing Function**
```sql
-- Check smoothing values are reasonable
SELECT * FROM ailog_peak.peak_statistics 
WHERE namespace = 'pcb-sit-01-app' 
ORDER BY day_of_week, hour_of_day, quarter_hour
LIMIT 20;
```

### ğŸ“Š Data Format Expected

```
Sample output from collect_peak_detailed.py:

day_of_week | hour_of_day | quarter_hour | namespace          | mean_errors | stddev_errors | samples
------------|-------------|--------------|-------------------|-------------|---------------|---------
0 (Mon)     | 8           | 0            | pcb-sit-01-app     | 203.32      | 45.67         | 3
0 (Mon)     | 8           | 15           | pcb-sit-01-app     | 195.45      | 42.15         | 3
0 (Mon)     | 8           | 30           | pcb-sit-01-app     | 187.23      | 40.89         | 3
...
```

### âš ï¸ IMPORTANT REMINDERS
- âœ… Use EXPLICIT dates (--from/--to), NOT --days for production
- âœ… Always include 'Z' suffix in timestamps
- âœ… Verify data count BEFORE deleting old DB
- âœ… Create backup/screenshot of table BEFORE DB load
- âœ… Check smoothing values make sense (not NaN, not negative)

---

**Ready for:** Next session - Execute Step 1-5 in order

---

## ğŸ“Š SESSION - 2025-12-16 10:30 UTC - Workspace Cleanup & Phase 5 Setup

### ğŸ¯ CÃ­l
VyÄistit workspace, archivovat starÃ© soubory, extrahovat dÅ¯leÅ¾itÃ© info.

### âœ… HOTOVO (10:30-11:00 UTC)

**1. Data Collection**
- âœ… collect_peak_detailed.py: 2025-12-15 (163,847 errors)
- âœ… collect_peak_detailed.py: 2025-12-01 (16 dnÃ­ zpÃ¡tky - CRITICAL)

**2. Scripts Cleanup**
- âœ… 8 core scripts v root (keep)
- âœ… 19 zastaralÃ½ch skriptÅ¯ â†’ _archive_scripts/
- âœ… 8 test_*.py skriptÅ¯ smazÃ¡no

**3. Documentation Cleanup**
- âœ… README_SCRIPTS.md aktualizovÃ¡n (8 core scripts)
- âœ… CONTEXT_RETRIEVAL_PROTOCOL.md aktualizovÃ¡n (Phase 5 status)
- âœ… VytvÃ¡Å™enÃ­ PHASE_ROADMAP.md (Phase 5-7 planning)

**4. Data/Backup Archivace**
- âœ… data/ â†’ /home/jvsete/git/sas/ai-data/
- âœ… 11 zastaralÃ½ch MD â†’ _archive_md/
- âœ… export_peak_statistics.py vytvoÅ™en

**5. Workspace Reorganizace**
- âœ… _archive_scripts/ (19 skriptÅ¯)
- âœ… _archive_md/ (11 dokumentÅ¯)
- âœ… ZbÃ½vÃ¡ 9 MD + 9 PY v root (clean!)

### ğŸ“Š VÃSLEDKY

| Item | PÅ™ed | Po | ZmÄ›na |
|------|------|----|----|
| Workspace | 618M | 404M | -214M |
| Root MD | 20+ | 9 | -11 (archivovÃ¡no) |
| Root PY | 35 | 9 | -26 (archivovÃ¡no) |
| Data soubory | 215M | v ai-data/ | archivovÃ¡no |

**AktivnÃ­ v root:**
- Scripts: collect_peak_detailed.py, fetch_unlimited.py, analyze_period.py, + 5 DB scripts
- Docs: working_progress.md, CONTEXT_RETRIEVAL_PROTOCOL.md, HOW_TO_USE.md, README_SCRIPTS.md, + 4 others

### ğŸ”„ NEXT PRIORITY (TODO)

**TODAY:**
- [ ] VyÄistit DB (DELETE starÃ© z peak_statistics)
- [ ] NahrÃ¡t novÃ¡ data do DB
- [ ] Verifikovat integritu

**NEXT:**
- [ ] VytvoÅ™it ingest_peak_statistics.py
- [ ] Phase 6a: DB schema validation
- [ ] Deploy to K8s


---

## ğŸ“ SESSION - 2025-12-16 11:00 UTC - Workspace Reorganization & Cleanup

### ğŸ¯ CÃ­l
VyÄistit workspace, reorganizovat scripty, aktualizovat dokumentaci.

### âœ… HOTOVO (11:00-11:15 UTC)

**1. Workspace Cleanup**
- âœ… ArchivovÃ¡no: copilot-chat-backups/ (5MB - backupy chatÅ¯, nepotÅ™ebnÃ©)
- âœ… ArchivovÃ¡no: updates/ (200KB - starÃ© session noty z listopadu)
- âœ… ArchivovÃ¡no: .backup_2025-11-18/ (1MB - starÃ½ backup, zastaralÃ½)
- âœ… ArchivovÃ¡no: tests/ (<1KB - prÃ¡zdnÃ½ folder)
- âœ… SmazÃ¡no: Dockerfile.peak-detector (experiment)
- âœ… SmazÃ¡no: Dockerfile.tmp (temporary file)
- âœ… SmazÃ¡no: __pycache__ (auto-generated)

**2. Scripts Reorganizace**
- âœ… VytvoÅ™en: `scripts/` folder s `scripts/INDEX.md` (detailnÃ­ reference)
- âœ… PÅ™esunuty: vÅ¡echny .py scripty (10 skriptÅ¯) â†’ scripts/
- âœ… PÅ™esunut: workflow_manager.sh â†’ scripts/
- âœ… ZachovÃ¡ny: references v dokumentaci

**3. MD Soubory Cleanup**
- âœ… ArchivovÃ¡no: COMPLETED_LOG.md (starÃ½ log)
- âœ… ArchivovÃ¡no: DEPLOYMENT.md (zastaralÃ©)
- âœ… ArchivovÃ¡no: HARBOR_DEPLOYMENT_GUIDE.md (reference v archÃ­vu)
- âœ… ArchivovÃ¡no: KNOWN_ISSUES_DESIGN.md (design doc)
- âœ… ArchivovÃ¡no: PHASE_ROADMAP.md (starÃ½ roadmap)
- âœ… ArchivovÃ¡no: README_SCRIPTS.md (nahrazeno scripts/INDEX.md)

**4. K8s Archivace**
- âœ… ArchivovÃ¡no: k8s/ folder (zastaralÃ© manifesty, cluster se jeÅ¡tÄ› Å™eÅ¡Ã­)

**5. Dokumentace Update**
- âœ… AktualizovÃ¡n: CONTEXT_RETRIEVAL_PROTOCOL.md (v2.1)
  - NovÃ¡ struktura s scripts/
  - ÄŒistÃ© workspace tree
  - Priority workflow Phase 5A (data ingestion)
  
### ğŸ“Š VÃSLEDKY CLEANUP

| Kategorie | PÅ™ed | Po | PoznÃ¡mka |
|-----------|------|----|----|
| Root MD files | 10 | 4 | -6 archivovÃ¡no |
| Root PY files | 9 | 0 | VÅ¡echny v scripts/ |
| Root folders | 14 | 9 | -5 do archÃ­vu |
| Total size | 283M | 283M | (venv zÅ¯stal) |
| Clean root | âŒ | âœ… | 4 MD + 7 config files |

**FinÃ¡lnÃ­ Root Struktura:**
```
ğŸ“„ README.md
ğŸ“„ CONTEXT_RETRIEVAL_PROTOCOL.md
ğŸ“„ working_progress.md
ğŸ“„ HOW_TO_USE.md
ğŸ“‚ scripts/                          â† ALL PRODUCTION CODE
ğŸ“‚ app/                              â† FastAPI app
ğŸ“‚ alembic/                          â† DB migrations
ğŸ“‚ _archive_md/                      â† Old docs (6 files)
ğŸ“‚ _archive_scripts/                 â† Old scripts (19 files)
ğŸ“‚ _archive_old/                     â† Old folders
ğŸ³ Dockerfile
ğŸ“¦ requirements.txt
... (config files)
```

### ğŸ”„ WHAT'S NEXT (Priority)

**IMMEDIATE (Phase 5A - Data Ingestion):**
1. [ ] Export peak_statistics to CSV (backup)
2. [ ] Verify current DB data
3. [ ] (Optional) Clean old DB records
4. [ ] Load new data if available
5. [ ] Verify data integrity

**NEXT SESSION:**
6. [ ] Create ingest_peak_statistics.py (automated loader)
7. [ ] Test full pipeline
8. [ ] Deploy to K8s (Phase 6)

### ğŸ“š KEY DOCUMENTS UPDATED

- `CONTEXT_RETRIEVAL_PROTOCOL.md` (v2.1) - Full workspace guide
- `scripts/INDEX.md` - Script reference + usage
- `working_progress.md` - This log

### ğŸ’¾ GIT STATUS

```bash
# Files to commit:
- CONTEXT_RETRIEVAL_PROTOCOL.md (updated)
- working_progress.md (this log)
- scripts/ folder structure (reorganized)
- _archive_*/ folders (new archiving)

# Not committing:
- .venv/, venv/ (env files)
- __pycache__/ (auto-generated, already in .gitignore)
```


---

## ğŸ—ºï¸ DÅ®LEÅ½ITÃ‰ LOKACE - Pro PÅ™Ã­Å¡tÃ­ Session

### ğŸ“ AktuÃ¡lnÃ­ K8s Konfigurace
```
Repo: /home/jvsete/git/sas/k8s-infra-apps-nprod/
Branch: feature/ai-log-analyzer-v2
Manifest: infra-apps/ai-log-analyzer/
Status: ZASTARALÃ - cluster se jeÅ¡tÄ› Å™eÅ¡Ã­, zatÃ­m ruÄnÄ›
```

### ğŸ“Š HistorickÃ¡ Data
```
Database: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
Schema: ailog_peak
Table: peak_statistics

Dates in DB:
- 2025-12-01 (initial load, 16 dnÃ­ zpÃ¡tky)
- 2025-12-15 (recent, 163,847 errors)

Query example:
SELECT date_trunc('day', measurement_time) as day, COUNT(*) 
FROM peak_statistics 
GROUP BY day 
ORDER BY day DESC;
```

### ğŸ’¾ ExportovanÃ©/Backup Data
```
Location: (needs export, see scripts/export_peak_statistics.py)
Format: CSV (YYYYMMDD_HHMMSS timestamp)
Command: cd scripts/ && python export_peak_statistics.py --from 2025-12-01 --to 2025-12-16
```

### ğŸ“ Archive Locations
```
_archive_md/          - Old documentation (6 files)
_archive_scripts/     - Old scripts Phase 1-3 (19 files)
_archive_old/         - Folders archived today:
                        â”œâ”€â”€ k8s/                 (zastaralÃ© manifesty)
                        â”œâ”€â”€ copilot-chat-backups/ (backupy chatÅ¯)
                        â”œâ”€â”€ updates/             (starÃ© session noty)
                        â”œâ”€â”€ .backup_2025-11-18/  (starÃ½ backup)
                        â””â”€â”€ tests/               (prÃ¡zdnÃ½ folder)
```

### ğŸ”‘ Key Contacts/Credentials (Cyberark)
```
Elasticsearch: XX_PCBS_ES_READ (elastic user)
Database: DAP_PCB safe (ailog_analyzer_user_d1)
Elasticsearch URL: elasticsearch-test.kb.cz:9500
```


---

## âœ… SESSION SUMMARY - 2025-12-16 11:00-11:30 UTC

### ğŸ¯ GOALS
- [x] Clean workspace structure
- [x] Organize all scripts into single folder
- [x] Update documentation
- [x] Commit changes to git

### ğŸ“Š COMPLETED
```
âœ… Workspace cleanup: 6 old folders archived
âœ… Scripts reorganization: 10 PY + 1 SH moved to scripts/
âœ… Created scripts/INDEX.md (detailed reference)
âœ… MD files reorganized: 6 archived
âœ… Documentation updated: CONTEXT_RETRIEVAL_PROTOCOL.md (v2.1)
âœ… Git commit: a857894 (Phase 5: Workspace cleanup & reorganization)
```

### ğŸ¯ FINAL WORKSPACE STRUCTURE
```
ai-log-analyzer/
â”œâ”€â”€ ğŸ“„ CONTEXT_RETRIEVAL_PROTOCOL.md  (v2.1) â† START HERE
â”œâ”€â”€ ğŸ“„ README.md                       (main docs)
â”œâ”€â”€ ğŸ“„ working_progress.md             (this log)
â”œâ”€â”€ ğŸ“„ HOW_TO_USE.md                   (tutorials)
â”‚
â”œâ”€â”€ ï¿½ï¿½ scripts/                        (ALL PRODUCTION CODE)
â”‚   â”œâ”€â”€ INDEX.md                       (script reference)
â”‚   â”œâ”€â”€ collect_peak_detailed.py       (â­ core)
â”‚   â”œâ”€â”€ fetch_unlimited.py
â”‚   â”œâ”€â”€ analyze_period.py
â”‚   â”œâ”€â”€ export_peak_statistics.py
â”‚   â”œâ”€â”€ verify_peak_data.py
â”‚   â”œâ”€â”€ init_peak_statistics_db.py
â”‚   â”œâ”€â”€ setup_peak_db.py
â”‚   â”œâ”€â”€ grant_permissions.py
â”‚   â”œâ”€â”€ create_known_issues_registry.py
â”‚   â””â”€â”€ workflow_manager.sh
â”‚
â”œâ”€â”€ ğŸ“‚ app/                            (FastAPI app)
â”œâ”€â”€ ğŸ“‚ alembic/                        (DB migrations)
â”œâ”€â”€ ğŸ“‚ _archive_md/                    (old docs, 6 files)
â”œâ”€â”€ ğŸ“‚ _archive_scripts/               (old scripts, 19 files)
â”œâ”€â”€ ğŸ“‚ _archive_old/                   (archived folders)
â”œâ”€â”€ ğŸ³ Dockerfile
â””â”€â”€ ğŸ“¦ requirements.txt
```

### ğŸ”„ NEXT PRIORITY - Phase 5A: DATA INGESTION

**Immediate tasks:**
1. [ ] Export current peak_statistics to CSV backup
2. [ ] Verify DB data integrity
3. [ ] Load new historical data (if available)
4. [ ] Test full pipeline

**See:** `scripts/INDEX.md` for exact commands

### ğŸ“ IMPORTANT FOR NEXT SESSION

**K8s Configuration Location:**
```
Repo: /home/jvsete/git/sas/k8s-infra-apps-nprod/
Branch: feature/ai-log-analyzer-v2
Manifest: infra-apps/ai-log-analyzer/
Status: ZASTARALÃ - cluster se Å™eÅ¡Ã­ pozdÄ›ji
```

**Historical Data Location:**
```
Database: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
Schema: ailog_peak
Table: peak_statistics

Current dates:
- 2025-12-01 (baseline, 16 days)
- 2025-12-15 (recent, 163,847 errors)
```

**To Check Status:**
```bash
cd /home/jvsete/git/sas/ai-log-analyzer
cat CONTEXT_RETRIEVAL_PROTOCOL.md    # Full context
cat scripts/INDEX.md                 # Scripts reference
tail -50 working_progress.md         # Last session log
```

### ï¿½ï¿½ GIT INFO
- Commit: a857894
- Branch: main
- Last commit message: "Phase 5: Workspace cleanup & reorganization"
- Status: âœ… Clean, ready for next work

---

**Session ended at:** 2025-12-16 11:30 UTC  
**Total cleanup time:** ~30 minutes  
**Files organized:** 46 changes in git commit  
**Workspace ready:** âœ… YES - Phase 5A ready to begin


---

## ğŸ” VYJASNÄšNÃ: Co je "peak_statistics" (vaÅ¾nÃ©!)

### âŒ Å PATNÃ‰ POCHOPENÃ
"peak_statistics" = statistika o peakech (events, detekce, atd.)

### âœ… SPRÃVNÃ‰ POCHOPENÃ
"peak_statistics" = **BASELINE PRO DETEKCI** peakÅ¯
- Je to reference data (znÃ¡mÃ¡/normÃ¡lnÃ­ stav)
- PouÅ¾Ã­vÃ¡ se pro porovnÃ¡nÃ­ = detekce anomÃ¡liÃ­

### ğŸ“Š OBSAH TABULKY peak_statistics
```
Å˜Ã¡dek = (den_tÃ½dne, hodina, Ätvrthodina, namespace)

PÅ™Ã­klad data:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Den  â”‚ Hod  â”‚ 15min  â”‚ Namespace     â”‚ PrÅ¯mÄ›r chyb  â”‚ StdDev chyb  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Pon  â”‚ 8:00 â”‚ 0      â”‚ pcb-sit-01    â”‚ 203          â”‚ 45           â”‚
â”‚ Pon  â”‚ 8:00 â”‚ 15     â”‚ pcb-sit-01    â”‚ 195          â”‚ 42           â”‚
â”‚ Pon  â”‚ 8:00 â”‚ 30     â”‚ pcb-sit-01    â”‚ 187          â”‚ 41           â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FORMULA PRO DETEKCI PEAKU:
  AktuÃ¡lnÃ­ chyby > (PrÅ¯mÄ›r + 3 * StdDev) = ANOMÃLIE!
  AktuÃ¡lnÃ­ chyby > (PrÅ¯mÄ›r + 5 * StdDev) = KRITICKÃ ANOMÃLIE!
```

### ğŸ“‹ TABULKY V DATABÃZI
```
ailog_peak schema obsahuje:

1. peak_raw_data         â† Raw data z Elasticsearch (15min okna)
                           PouÅ¾Ã­vÃ¡ se pro vÃ½poÄet baseline

2. peak_statistics       â† BASELINE (prÅ¯mÄ›r + stddev)
                           â­ TO CO VÃS ZAJÃMÃ!
                           PouÅ¾Ã­vÃ¡ se pro detekci anomÃ¡liÃ­

3. peak_history          â† HistorickÃ© peaky (skuteÄnÃ© detekovanÃ© anomÃ¡lie)
                           Peaky co se skuteÄnÄ› staly

4. active_peaks          â† AktuÃ¡lnÄ› bÄ›Å¾Ã­cÃ­ peaky
                           Real-time detekce
```

### ğŸ¯ PROÄŒ TEN NÃZEV?
- PÅ¯vodnÄ› by to mÄ›lo bÃ½t: `error_baseline` nebo `anomaly_thresholds`
- Ale v kÃ³du se to tak jmenuje, tak to nechÃ¡me
- **DÅ®LEÅ½ITÃ‰:** VÄ›dÄ›t, Å¾e to je BASELINE, ne samotnÃ© peaky!

### ğŸ’¾ AKTUÃLNÃ DATA V DB (2025-12-16)
```
Tabulka: peak_statistics (schema: ailog_peak)
Status: âœ… NaÄtena data pro:
  - 2025-12-01 (baseline, historickÃ© 16 dnÃ­)
  - 2025-12-15 (recent, 163,847 errors)

OvÄ›Å™it stav:
  psql -h P050TD01.DEV.KB.CZ -U ailog_analyzer_user_d1 -d ailog_analyzer
  SELECT COUNT(*) FROM ailog_peak.peak_statistics;
  SELECT * FROM ailog_peak.peak_statistics LIMIT 5;
```

