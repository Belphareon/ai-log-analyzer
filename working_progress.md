# üîÑ Working Progress - AI Log Analyzer (AKTU√ÅLN√ç)

**Projekt:** AI Log Analyzer - Phase 5A (Data Ingestion)  
**Posledn√≠ update:** 2025-12-16 13:10 UTC  
**Status:** üêõ TIMEZONE BUG FOUND & FIXED - Re-collecting with correction

---

## üìä CURRENT STATUS

### ‚úÖ COMPLETED TODAY

| Task | Status | Details |
|------|--------|---------|
| Smazat testovac√≠ data z DB | ‚úÖ | 186 rows deleted |
| Vytvo≈ôit `ingest_from_log.py` | ‚úÖ | Script created & tested |
| Aktualizovat `scripts/INDEX.md` | ‚úÖ | Full workflow documented |
| Spustit sb√≠r√°n√≠ 2025-12-01 (v1) | ‚úÖ | Jen 5 patterns - BUG FOUND |
| **BUG: Sb√≠r√°n√≠ jen 5 patterns** | üêõ FOUND | `print_detailed_report()` limited output |
| **FIX: Oprava collect_peak_detailed.py** | ‚úÖ | Removed `[:5]` limit - ALL patterns |
| **Ingest 2025-12-01 (v1)** | ‚úÖ | 186 rows loaded BUT timezone offset -1h! |
| **TIMEZONE BUG FOUND** | üêõ FOUND | Data in DB shifted -1 hour vs reality |
| **ROOT CAUSE:** | üîç | Using `win_end.hour` instead of `win_start.hour` |
| **FIX: Timezone correction** | ‚úÖ | Changed to `win_start.weekday()`, `win_start.hour` |
| **Re-collecting 2025-12-01** | ‚úÖ | PID 30444 - RUNNING with fix |

## üîß SMOOTHING ALGORITHM (TO IMPLEMENT)

**Goal:** Detect real peaks by smoothing outliers using 3-window + cross-day aggregation

**Algorithm:**
```
For each time bucket (day_of_week, hour, quarter, namespace):

1. HORIZONTAL SMOOTHING (same day):
   - Take current + adjacent time windows (¬±2 = 5 windows total)
   - Calculate average: smooth_h = mean(win[i-2:i+3])
   
2. VERTICAL SMOOTHING (same time, different days):
   - For SAME time bucket from 3+ previous days
   - Calculate average: smooth_v = mean(day1, day2, day3)
   
3. COMBINE:
   - final_mean = (smooth_h + smooth_v) / 2
   - If only 1 day available: use only smooth_h
   - If no adjacent windows: use smooth_h with available neighbors
```

**Example (as user specified):**
```
Day 1 (2025-12-01):
  13:30 = 25, 13:45 = 4, 14:00 = 51, 14:15 = 9, 14:30 = 13433, 14:45 = 41303
  After smoothing:
    14:30 = (25+4+51+9+13433)/5=2704 (horizontal) 
           + later cross-day data (vertical)

Day 2-3: Will add vertical smoothing when available
```

**Current Status:** Pending - need 3+ days of data first

**Problem:**
- ES shows peak at **14:00:00 UTC (81,171 errors)** for pcb-dev-01-app on 2025-12-01
- DB stores same peak as **hour=13 (41,303 mean_errors)**
- **ALL data stored with -1 hour offset**

**Root Cause Investigation:**
1. Changed `collect_peak_detailed.py` from `win_end` to `win_start` for hour calculation
2. **BUT:** Data collected after change show SAME offset (-1 hour)
3. **CONCLUSION:** Either:
   - Python cache still running old code, OR
   - Bug is in `group_into_windows()` or timestamp parsing from ES

**Workaround Solution (IMMEDIATE):**
- FIX: Add +1 hour offset in `ingest_from_log.py` when parsing
- This corrects all data being inserted to DB
- Will apply to parser: `hour_of_day = (hour_of_day + 1) % 24`

**Root Cause Fix (LATER):**
- Debug `collect_peak_detailed.py` with print statements
- Verify windows are generated correctly
- Check ES timestamp parsing
- May need to re-run collection AFTER confirming fix works

### üîÑ CURRENTLY RUNNING

```
Terminal (Background):
  PID:     30444 (was 30443)
  Command: collect_peak_detailed.py --from "2025-12-01T00:00:00Z" --to "2025-12-02T00:00:00Z"
  Output:  /tmp/peak_fixed_2025_12_01.txt (BUILDING)
  Status:  ‚è≥ COLLECTING (WITH TIMEZONE FIX)
  
NEXT STEPS:
  1. ‚úÖ Check if PID still running: ps aux | grep 30444
  2. ‚úÖ When done: grep -c "^   Pattern " /tmp/peak_fixed_2025_12_01.txt
  3. ‚úÖ Ingest: python ingest_from_log.py --input /tmp/peak_fixed_2025_12_01.txt
  4. ‚úÖ Verify: SELECT * FROM peak_statistics WHERE hour_of_day IN (14,15) LIMIT 5
```

### üìã TODO NEXT (PRIORITY ORDER)

```
PHASE 1 (IMMEDIATE - OFFSET FIX):
  [ ] 1. Smazat star√° data z DB: DELETE FROM peak_statistics
  [ ] 2. Opravit ingest_from_log.py: +1 hour offset p≈ôi parsov√°n√≠
  [ ] 3. Re-ingest 2025-12-01 data s korekc√≠
  [ ] 4. OVƒö≈òIT: DB m√° teƒè hour=14 m√≠sto hour=13 pro biggest peak
  [ ] 5. Commitnout parser fix

PHASE 2 (DEBUG & PERMANENT FIX):
  [ ] 6. Debug collect_peak_detailed.py - zjistit kde se -1h tvo≈ô√≠
  [ ] 7. P≈ôidat debug prints k windows a timestamps
  [ ] 8. Re-run collection s debug outputem
  [ ] 9. Naj√≠t root cause a opravit nav≈ædy

PHASE 3 (SMOOTHING):
  [ ] 10. Vy≈ôe≈°it stddev_errors (teƒè je v≈ædy 0)
  [ ] 11. Opravit UPSERT pro agregaci v√≠ce dn√≠

PHASE 4 (CONTINUE COLLECTION):
  [ ] 12. Sb√≠r√°n√≠ 2025-12-02 & 2025-12-03 (s opravou offsetu)
  [ ] 13. Sb√≠r√°n√≠ zbyl√Ωch dn√≠ (6 batch≈Ø)
```

---

## üíæ DATA FILES

| File | Status | Notes |
|------|--------|-------|
| `/tmp/peak_full_2025_12_01.txt` | ‚ùå DELETED | v1 - had 186 patterns BUT with -1h offset |
| `/tmp/peak_fixed_2025_12_01.txt` | ‚è≥ COLLECTING | v2 - WITH TIMEZONE FIX (PID 30444) |
| `/tmp/peak_full_2025_12_02_03.txt` | üìã TODO | |

---

## üîß COMMITS

```
Current Branch: main
Recent commits:
  - (pending) Timezone fix: Use win_start instead of win_end
  - e9b0280    Phase 5: Session complete - 2025-12-01 data loaded (186 patterns)
  - 0e83956    Status update
  - 5996374    Phase 5: Fix collect_peak_detailed.py to output ALL patterns
```

## üö® PRAVIDLA

‚ö†Ô∏è **NE RU≈†IT Bƒö≈Ω√çC√ç PROCES** - Sb√≠r√°n√≠ trv√° 2-3 minuty!  
‚ö†Ô∏è **PRACUJ V JIN√âM TERMIN√ÅLU** - Nech PID 30070 b√Ωt!  
‚ö†Ô∏è **V≈ΩDYCKY EXPLICIT DATES** - `--from "2025-12-XXT00:00:00Z" --to "2025-12-YYT00:00:00Z"`  
‚ö†Ô∏è **Z SUFFIX** - Elasticsearch pot≈ôebuje Z, ne +00:00  

---

## üîë KEY INFO

**DB:**
- Host: P050TD01.DEV.KB.CZ:5432
- DB: ailog_analyzer
- Table: ailog_peak.peak_statistics
- Current rows: 5 (star√° data - bude se p≈ôepsat)
- Expected after 2025-12-01 load: 384 rows

**Scripts Updated:**
- `collect_peak_detailed.py` - ‚úÖ FIXED (output ALL patterns)
- `ingest_from_log.py` - ‚úÖ WORKS
- `scripts/INDEX.md` - ‚úÖ UPDATED

**Git Commit:**
- SHA: 5996374
- Msg: "Phase 5: Fix collect_peak_detailed.py to output ALL patterns"

**Archiv star≈°√≠ch log≈Ø:** `_archive_md/COMPLETED_LOG_2025_12_16.md`
