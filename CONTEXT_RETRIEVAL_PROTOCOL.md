# CONTEXT RETRIEVAL PROTOCOL
## AI Log Analyzer Project - Kontext pro Kontinuitu PrÃ¡ce

**Verze:** 2.0  
**Datum vytvoÅ™enÃ­:** 2025-12-12  
**PoslednÃ­ update:** 2025-12-16  
**ÃšÄel:** RychlÃ© naÄtenÃ­ kontextu pro pokraÄovÃ¡nÃ­ v prÃ¡ci na projektu

---

## ğŸ“‹ PROJEKT OVERVIEW

### Co je AI Log Analyzer?
- **ÃšÄel:** AutomatickÃ¡ analÃ½za logÅ¯ z Elasticsearch (K8s aplikace)
- **Funkce:** Detekce anomÃ¡liÃ­, clustering chybovÃ½ch vzorÅ¯, temporÃ¡lnÃ­ analÃ½za
- **Technologie:** FastAPI + PostgreSQL + Elasticsearch + Ollama (optional)
- **Deployment:** Kubernetes (ArgoCD) + Harbor registry
- **Stav:** Phase 4 COMPLETE âœ… | Phase 5 IN PROGRESS ğŸ”„ - Peak Detection Baseline

---

## ğŸ¯ AKTUÃLNÃ STAV (2025-12-16 - Phase 5 IN PROGRESS)

### âœ… HOTOVO (Phase 4 + 5 start)
1. **Docker Image** âœ…
   - Tag: `v0.4.0-docker-verified` + `latest`
   - Registry: `dockerhub.kb.cz/pccm-sq016/ai-log-analyzer`

2. **K8s Manifests** âœ…
   - Location: `/home/jvsete/git/sas/k8s-infra-apps-nprod/infra-apps/ai-log-analyzer/`

3. **Database Schema** âœ…
   - PostgreSQL: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
   - Schema: ailog_peak

4. **Phase 5: Peak Collection Started** âœ…
   - âœ… collect_peak_detailed.py spuÅ¡tÄ›n pro 2025-12-15 (163,847 errors)
   - âœ… collect_peak_detailed.py spuÅ¡tÄ›n pro 2025-12-01 (16 dnÃ­ zpÃ¡tky - CRITICAL)
   - âœ… ArchivovÃ¡ny starÃ© scripty (19 v _archive_scripts/)
   - âœ… SmazÃ¡ny test_*.py scripty (8 testÅ¯)
   - âœ… AktualizovÃ¡n README_SCRIPTS.md

### ğŸ”„ V PROCESU (Phase 5 - Current)
1. **Data Ingestion Pipeline**
   - [ ] Exportovat data do CSV tabulky
   - [ ] VyÄistit DB (DELETE starÃ© zÃ¡znamy)
   - [ ] NahrÃ¡t novÃ¡ data do peak_statistics
   - [ ] Verifikace pÅ™es verify_peak_data.py

2. **Documentation Cleanup**
   - [x] Archivovat starÃ© scripty
   - [x] Aktualizovat README_SCRIPTS.md
   - [ ] Aktualizovat CONTEXT_RETRIEVAL_PROTOCOL.md (TEN SOUBOR - IN PROGRESS)
   - [ ] Archivovat starÃ© MD soubory

### ğŸ“‹ TODO (Next Priority)
1. VytvoÅ™it `ingest_peak_statistics.py` skript
2. Dokumentovat novÃ½ script
3. Deploy to K8s
4. Test integration

---

## ğŸ“ STRUKTURA PROJEKTU

### KlÃ­ÄovÃ© Soubory (Phase 5)
```
ai-log-analyzer/
â”œâ”€â”€ collect_peak_detailed.py          # â­ CORE - SbÃ­rÃ¡ peak data
â”œâ”€â”€ fetch_unlimited.py                # â­ CORE - ES fetcher
â”œâ”€â”€ analyze_period.py                 # Orchestrator
â”œâ”€â”€ export_peak_statistics.py         # Export do CSV
â”œâ”€â”€ init_peak_statistics_db.py        # DB init (1x)
â”œâ”€â”€ setup_peak_db.py                  # DB setup (1x)
â”œâ”€â”€ verify_peak_data.py               # Verifikace
â”œâ”€â”€ grant_permissions.py              # DB perms (1x)
â”œâ”€â”€ create_known_issues_registry.py   # Registry
â”œâ”€â”€ working_progress.md               # âœ… SESSION LOG
â”œâ”€â”€ CONTEXT_RETRIEVAL_PROTOCOL.md     # âœ… REFERENCE
â”œâ”€â”€ README_SCRIPTS.md                 # âœ… SCRIPT DOCS
â”œâ”€â”€ PHASE_ROADMAP.md                  # âœ… ROADMAP
â”œâ”€â”€ HOW_TO_USE.md                     # âœ… USER GUIDE
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ Dockerfile                        # Docker build
â””â”€â”€ docker-compose.yml                # Local dev
```

### Git Struktura
```
/home/jvsete/git/sas/
â”œâ”€â”€ ai-log-analyzer/                           # Development workspace
â””â”€â”€ k8s-infra-apps-nprod/                      # Production K8s manifests
    â””â”€â”€ infra-apps/ai-log-analyzer/            # â† Deploy location
        â””â”€â”€ feature/ai-log-analyzer-v2         # â† Active branch
```

---

## ğŸ”‘ KLÃÄŒOVÃ‰ INFORMACE

### Credentials (Cyberark)
- **Elasticsearch:** XX_PCBS_ES_READ (elastic user)
- **Database:** DAP_PCB safe (ailog_analyzer_user_d1)
- **URL:** elasticsearch-test.kb.cz:9500

### Network Config
- **DNS (Prod):** ai-log-analyzer.sas.kbcloud
- **DNS (Test):** ai-log-analyzer-test.sas.kbcloud
- **Tenant Network:** 10.85.88.128/25
- **DNS Resolver:** 10.85.88.1

### Database Connection
- **Host:** P050TD01.DEV.KB.CZ (TODO: verify NPROD host)
- **Port:** 5432
- **Database:** ailog_analyzer
- **Schema:** public (tables: known_errors, analysis_runs, etc.)

### Elasticsearch Indices (Phase 5 - AKTUÃLNÃ)
- **Active:** `cluster-app_pcb-*,cluster-app_pca-*,cluster-app_pcb-ch-*`
- ~~Old~~ `logstash-kb-k8s-apps-nprod-*`, ~~`logstash-kb-k8s-apps-prod-*`~~
- **Env var:** `ES_INDEX` (POZOR: byl chybnÄ› `ES_INDICES`!)
- **Fields:** message, app_name, level, @timestamp, kubernetes.namespace

---

## ğŸ› ï¸ WORKFLOW: Jak PokraÄovat

### 1. PÅ™ed ZaÄÃ¡tkem PrÃ¡ce
```bash
# NaÄti aktuÃ¡lnÃ­ stav
cat /home/jvsete/git/sas/ai-log-analyzer/working_progress.md

# Zkontroluj git branch
cd /home/jvsete/git/sas/k8s-infra-apps-nprod
git status
git branch  # MÄ›l bys bÃ½t na feature/ai-log-analyzer-v2

# Zkontroluj Docker image v Harbor
# (pokud potÅ™ebujeÅ¡ rebuild)
```

### 2. PrÃ¡ce na ZmÄ›nÃ¡ch
```bash
# Development workspace
cd /home/jvsete/git/sas/ai-log-analyzer

# TestovÃ¡nÃ­ lokÃ¡lnÄ› (pokud potÅ™eba)
python -m pytest tests/

# Build novÃ©ho image (pokud zmÄ›ny v Dockerfile)
podman build -t ai-log-analyzer:latest .
```

### 3. Update K8s Manifests
```bash
# Copy zmÄ›nÄ›nÃ© manifesty
cp k8s-manifests-v2/* /home/jvsete/git/sas/k8s-infra-apps-nprod/infra-apps/ai-log-analyzer/

# Git commit
cd /home/jvsete/git/sas/k8s-infra-apps-nprod
git add infra-apps/ai-log-analyzer/
git commit -m "Update: [popis zmÄ›ny]"
git push origin feature/ai-log-analyzer-v2
```

### 4. Deployment (ArgoCD)
```bash
# ArgoCD automaticky detekuje zmÄ›ny v gitu
# Manual sync (pokud potÅ™eba):
argocd app sync ai-log-analyzer

# Monitor deployment
kubectl get pods -n ai-log-analyzer -w
kubectl logs -n ai-log-analyzer deployment/ai-log-analyzer -f
```

### 5. Update Progress Log
```bash
# VÅ¾dy aktualizuj working_progress.md s timestampem
echo "## ğŸ“‹ $(date +%Y-%m-%d) - [Popis prÃ¡ce]" >> working_progress.md
echo "" >> working_progress.md
echo "### Co bylo udÄ›lÃ¡no" >> working_progress.md
echo "- [ ] Todo item 1" >> working_progress.md
```

---

## ğŸ“ KONVENCE PRO LOGGING

### Timestamp Format
```
## ğŸ“‹ YYYY-MM-DD HH:MM UTC - [Titulek session]
```

### Session Structure
```markdown
## ğŸ“‹ 2025-12-12 14:30 UTC - Feature X Implementation

### ğŸ¯ CÃ­l
- Co chci udÄ›lat

### âœ… Hotovo
- [x] Item 1 (14:35 UTC)
- [x] Item 2 (14:42 UTC)

### ğŸ”„ V Procesu
- [ ] Item 3 (started 14:50 UTC)

### âš ï¸ ProblÃ©my
- Popis problÃ©mu + jak byl vyÅ™eÅ¡en

### ğŸ“Š VÃ½sledek
- Stav po ukonÄenÃ­ session
```

---

## ğŸš¨ ZNÃMÃ‰ PROBLÃ‰MY & Å˜EÅ ENÃ

### Problem 1: Docker Hub Rate Limit
**Symptom:** `You have reached your pull rate limit`  
**Solution:** PoÄkat 6 hodin nebo pouÅ¾Ã­t Docker auth token  
**Workaround:** `podman build --network=host`

### Problem 2: WSL2 Docker Network Corruption
**Symptom:** `netavark: unable to append rule`  
**Solution:** `sudo nft flush ruleset` + delete orphaned chains  
**Workaround:** `docker run --network none` pro lokÃ¡lnÃ­ testy

### Problem 3: Database Host DEV vs NPROD
**Symptom:** ConfigMap has `P050TD01.DEV.KB.CZ`  
**Status:** TODO - verify correct NPROD host  
**Action:** Check with DevOps if DEV host is correct for NPROD cluster

### Problem 4: Soteri PASSWORD_IN_URL
**Symptom:** Secret obsahuje password v URL stringu  
**Solution:** âœ… RESOLVED - Build connection string v Pythonu, ne v ENV  
**Status:** Clean scan âœ…

---

## ğŸ“š ACTIVE DOCUMENTATION (Updated 2025-12-16)

### â­ POUÅ½ÃVEJ TYTO (PRIMARY):
1. **working_progress.md** - Session log + TODO (MAIN!)
2. **CONTEXT_RETRIEVAL_PROTOCOL.md** - Ten soubor (reference)
3. **README_SCRIPTS.md** - 8 core skriptÅ¯ (UPDATED 2025-12-16!)
4. **HOW_TO_USE.md** - User guide

### ğŸ—‚ï¸ ARCHIVED / ZASTARALÃ‰ (IGNORUJ):
- MASTER.md (2025-12-02)
- README_v2.md
- ORCHESTRATION_PROGRESS.md (2025-12-08)
- working_progress_backup_* (nepouÅ¾Ã­vej!)
- Viz: **MD_REGISTRY.md** pro ÃºplnÃ½ seznam

### ğŸ“– Pro Development:
- **HARBOR_DEPLOYMENT_GUIDE.md** - K8s deployment
- **KNOWN_ISSUES_DESIGN.md** - Known issues design

---

## ğŸ¯ NEXT STEPS - Phase 5 Workflow (Priority)

**IMMEDIATE (TODAY - 2025-12-16):**
1. [ ] Exportovat vÃ½stupy collect_peak_detailed.py do CSV tabulky
2. [ ] VyÄistit DB - DELETE starÃ© zÃ¡znamy z peak_statistics
3. [ ] NahrÃ¡t novÃ¡ data do DB (INSERT)
4. [ ] Verifikovat pÅ™es verify_peak_data.py

**NEXT SESSION:**
5. [ ] VytvoÅ™it `ingest_peak_statistics.py` skript (JSON â†’ DB loader)
6. [ ] Dokumentovat v README_SCRIPTS.md
7. [ ] Archivovat starÃ© MD soubory (_archive_md/)

**FINAL (Deployment):**
8. [ ] Deploy to K8s cluster nprod-3100
9. [ ] Test health endpoint
10. [ ] Verify integration

---

## âœ… CHECKLIST: NÃ¡vrat k Projektu

KdyÅ¾ zaÄÃ­nÃ¡Å¡ novou session:

- [ ] PÅ™eÄti poslednÃ­ entry v `working_progress.md`
- [ ] Zkontroluj git branch: `feature/ai-log-analyzer-v2`
- [ ] OvÄ›Å™ aktuÃ¡lnÃ­ stav K8s deploymentu (pokud nasazeno)
- [ ] NaÄti tento CONTEXT_RETRIEVAL_PROTOCOL.md
- [ ] VytvoÅ™ novÃ½ entry v progress s timestampem
- [ ] Postupuj po malÃ½ch krocÃ­ch, loguj prÅ¯bÄ›Å¾nÄ›

---

## ğŸ“Š Scripts Registry (2025-12-16)

**8 CORE SCRIPTS (v root - AKTIVNÃ):**
- `collect_peak_detailed.py` â­ - SbÃ­rÃ¡ peak data z ES
- `fetch_unlimited.py` â­ - ES fetcher (dependency)
- `analyze_period.py` - Full orchestrator A-Z
- `init_peak_statistics_db.py` - DB init (1x setup)
- `setup_peak_db.py` - DB setup helper (1x)
- `verify_peak_data.py` - DB verification
- `grant_permissions.py` - DB permissions (1x)
- `create_known_issues_registry.py` - Known issues

**19 ARCHIVED (v _archive_scripts/ - NEPOUÅ½ÃVEJ):**
- StarÃ© fetch family (fetch_errors.py, fetch_simple.py, atd.)
- ZastaralÃ© analyzery (analyze_daily.py, intelligent_analysis.py)
- StarÃ© peak collection (collect_historical_peak_data.py, atd.)
- Diagnostic scripty (diagnose_es_data.py, check_es_indices.py, atd.)
- Trace legacy (trace_extractor.py, trace_report_detailed.py)

â†’ **Detaily:** Viz `README_SCRIPTS.md`

---

**Last Updated:** 2025-12-16 10:30 UTC  
**Maintainer:** AI Assistant + jvsete  
**Status:** âœ… Phase 4 Complete | ğŸ”„ Phase 5 IN PROGRESS - Peak Detection Baseline Collection
