# CONTEXT RETRIEVAL PROTOCOL
## AI Log Analyzer - Quick Reference

**Verze:** 2.2  
**Last Update:** 2025-12-17  
**√öƒçel:** Rychl√Ω p≈ôehled pro pokraƒçov√°n√≠ v pr√°ci

---

## üìã CO TO JE

**AI Log Analyzer** - Automatick√° anal√Ωza error≈Ø z Elasticsearch s AI doporuƒçen√≠mi

- **Tech Stack:** Python + FastAPI + PostgreSQL + Elasticsearch + Ollama (optional)
- **Deployment:** Kubernetes (ArgoCD) + Harbor registry  
- **Current Phase:** Phase 5 (Peak Detection Baseline)

---

## üéØ AKTU√ÅLN√ç STAV (2025-12-16 11:00 UTC - Phase 5 IN PROGRESS)

### ‚úÖ HOTOVO (Phase 4 + 5 setup)
1. **Docker Image** ‚úÖ
   - Tag: `v0.4.0-docker-verified` + `latest`
   - Registry: `dockerhub.kb.cz/pccm-sq016/ai-log-analyzer`

2. **Database Schema** ‚úÖ
   - PostgreSQL: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
   - Schema: ailog_peak

3. **Phase 5: Peak Data Collection** ‚úÖ
   - ‚úÖ collect_peak_detailed.py: 2025-12-01 (230K errors, ready for load)
   - ‚úÖ Scripts reorganized do `scripts/` s `scripts/INDEX.md`
   - ‚úÖ Workspace cleanup (6 archiv≈Ø)
   - ‚ö†Ô∏è DB: testovac√≠ data (2025-12-05) - BUDOU SMAZANA

### üîÑ V PROCESU (Phase 5B - Priority: DATA INGESTION)

**Production Data Status:**
```
2025-12-01: ‚úÖ 230,146 errors (4 namespaces: pcb-*)
            Lokace: /tmp/peak_data_2025_12_01.txt
            ‚ö†Ô∏è Chyb√≠: pca-dev, pca-sit

2025-12-02 a≈æ 2025-12-15: ‚ùå CHYB√ç - Nutno st√°hnout
2025-12-16: ‚è≥ TODAY - Je≈°tƒõ se sb√≠r√°
```

**DB Current State:**
```
‚ùå Testovac√≠ data (budou smazana):
   - 2,623 rows z 2025-12-05
   - 6 namespaces (pca-* + pcb-*)
   - Status: TO DELETE
```

**Next 5 Steps (PRIORITY ORDER):**
1. [ ] **Smazat** testovac√≠ data z DB
2. [ ] **St√°hnout** chybƒõj√≠c√≠ data 2025-12-02 a≈æ 2025-12-15
3. [ ] **Ovƒõ≈ôit** form√°t dat z 2025-12-01
4. [ ] **Nata≈æ** v≈°ech dat do DB (s smoothingem)
5. [ ] **Validovat** kompletn√≠ range 2025-12-01 a≈æ 2025-12-15

### üìã NEXT (Priority Order)
1. ‚è≠Ô∏è Load data into DB - Phase 5B (THIS PRIORITY!)
2. ‚è≠Ô∏è Create ingest_peak_statistics.py
3. ‚è≠Ô∏è Deploy to K8s cluster (Phase 6)
4. ‚è≠Ô∏è Cluster automate (Phase 7)

---

## üéØ PEAK DETECTION - KL√çƒåOV√â PO≈ΩADAVKY

**PROƒå:** Zjistit CO SE DƒöJE p≈ôi anom√°li√≠ch, detekovat, analyzovat, vy≈ôe≈°it

**LOGIKA VKL√ÅD√ÅN√ç DAT:**
```
6 REFERENƒåN√çCH OKEN:
  - 3 okna P≈òED (stejn√Ω den): time-45min, time-30min, time-15min
  - 3 okna stejn√Ω ƒças (jin√Ω den): den-1, den-2, den-3

Kombinovan√° reference:
  reference = (avg(3_pred) + avg(3_dny)) / 2

IF nova_hodnota >= 10√ó reference:
  ‚Üí Oznaƒçit jako PEAK
  ‚Üí VYNECHAT z DB (nezapisovat)
  ‚Üí Zapsat do LOG pro anal√Ωzu
  
ELSE:
  ‚Üí Zapsat norm√°lnƒõ do DB

Special cases:
  - Pokud reference < 10: threshold 50√ó (ne 10√ó)
  - Pokud hodnota < 10: NIKDY skip (baseline)
  - Nen√≠ t≈ôeba v√≠ce dn√≠! Staƒç√≠ i 2 dny (Thu+Fri) proto≈æe lze v≈ædy naj√≠t 3 okna p≈ôed + reference Den-1
```

**OUTPUT:**
1. **DB (peak_statistics):** Pouze norm√°ln√≠ provoz (bez peaks)
2. **LOG (peaks_analysis.log):** V≈°echny peaks s kontextem pro anal√Ωzu
   - Timestamp, namespace, hodnota, baseline, ratio
   - ¬± 30min kontext (co se dƒõlo p≈ôed/po)

**VERIFIKACE:**
- TOP 20 hodnot v DB < 1000 (peaks skipnuty)
- Baseline hodnoty (2-65) v DB p≈ô√≠tomny
- Peaks (2890, 43000, atd.) v logu, NE v DB

---

## üìÅ WORKSPACE STRUKTURA (2025-12-16)

```
ai-log-analyzer/
‚îú‚îÄ‚îÄ üìÑ README.md                      # ‚≠ê Main documentation
‚îú‚îÄ‚îÄ üìÑ CONTEXT_RETRIEVAL_PROTOCOL.md  # ‚≠ê This file - quick context
‚îú‚îÄ‚îÄ üìÑ working_progress.md            # ‚≠ê Session log + tasks
‚îú‚îÄ‚îÄ üìÑ HOW_TO_USE.md                  # ‚≠ê User guide + examples
‚îÇ
‚îú‚îÄ‚îÄ üìÇ scripts/                       # ALL PRODUCTION SCRIPTS
‚îÇ   ‚îú‚îÄ‚îÄ INDEX.md                      # üìã Script reference (START HERE!)
‚îÇ   ‚îú‚îÄ‚îÄ collect_peak_detailed.py      # ‚≠ê CORE - Peak data collector
‚îÇ   ‚îú‚îÄ‚îÄ fetch_unlimited.py            # Elasticsearch fetcher
‚îÇ   ‚îú‚îÄ‚îÄ analyze_period.py             # Orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ export_peak_statistics.py     # CSV export
‚îÇ   ‚îú‚îÄ‚îÄ verify_peak_data.py           # Data validation
‚îÇ   ‚îú‚îÄ‚îÄ init_peak_statistics_db.py    # DB init (1x)
‚îÇ   ‚îú‚îÄ‚îÄ setup_peak_db.py              # DB setup (1x)
‚îÇ   ‚îú‚îÄ‚îÄ grant_permissions.py          # DB perms (1x)
‚îÇ   ‚îú‚îÄ‚îÄ create_known_issues_registry.py # Known issues
‚îÇ   ‚îî‚îÄ‚îÄ workflow_manager.sh           # Shell wrapper
‚îÇ
‚îú‚îÄ‚îÄ üìÇ app/                           # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ main.py                       # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ alembic/                       # Database migrations
‚îÇ   ‚îú‚îÄ‚îÄ versions/
‚îÇ   ‚îî‚îÄ‚îÄ env.py
‚îÇ
‚îú‚îÄ‚îÄ üìÇ _archive_md/                   # OLD Documentation (ignore)
‚îÇ   ‚îú‚îÄ‚îÄ COMPLETED_LOG.md
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ HARBOR_DEPLOYMENT_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ KNOWN_ISSUES_DESIGN.md
‚îÇ   ‚îú‚îÄ‚îÄ PHASE_ROADMAP.md
‚îÇ   ‚îî‚îÄ‚îÄ README_SCRIPTS.md
‚îÇ
‚îú‚îÄ‚îÄ üìÇ _archive_scripts/              # OLD Scripts from Phase 1-3
‚îÇ   ‚îî‚îÄ‚îÄ (19 zastaral√Ωch skript≈Ø)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ _archive_old/                  # OLD Folders (cleanup 2025-12-16)
‚îÇ   ‚îú‚îÄ‚îÄ k8s/                          # Zastaral√© manifesty
‚îÇ   ‚îú‚îÄ‚îÄ copilot-chat-backups/         # Chat backupy
‚îÇ   ‚îú‚îÄ‚îÄ updates/                      # Star√© session noty
‚îÇ   ‚îú‚îÄ‚îÄ .backup_2025-11-18/           # Star√Ω backup
‚îÇ   ‚îî‚îÄ‚îÄ tests/                        # Pr√°zdn√Ω test folder
‚îÇ
‚îú‚îÄ‚îÄ üê≥ Dockerfile                     # Current image build
‚îú‚îÄ‚îÄ üì¶ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ üìã docker-compose.yml             # Dev environment
‚îú‚îÄ‚îÄ üîë .env                           # Configuration (git-ignored)
‚îú‚îÄ‚îÄ alembic.ini                       # DB migration config
‚îú‚îÄ‚îÄ pyproject.toml                    # Python project config
‚îî‚îÄ‚îÄ .gitignore                        # Git ignore rules
```

---

## üîë KL√çƒåOV√â INFORMACE

### Database Connection
```
Host: P050TD01.DEV.KB.CZ
Port: 5432
Database: ailog_analyzer
Schema: ailog_peak (tables: peak_statistics, peak_raw_data, etc.)

USERS:
- ailog_analyzer_user_d1     ‚Üí Data operations (SELECT, INSERT, UPDATE, DELETE)
- ailog_analyzer_ddl_user_d1 ‚Üí DDL operations (CREATE, ALTER, DROP, GRANT)

ROLES:
- role_ailog_analyzer_ddl    ‚Üí DDL role (SET ROLE p≈ôed DDL operacemi)
```

**KRITICKY D≈ÆLE≈ΩIT√â - Jak se p≈ôipojit k DB:**

**1. DATA OPERACE (SELECT, INSERT, UPDATE, DELETE):**
```python
from dotenv import load_dotenv
import psycopg2
import os

load_dotenv()  # ‚ö†Ô∏è POVINN√â! Naƒçte .env soubor

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'P050TD01.DEV.KB.CZ'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'ailog_analyzer'),
    'user': os.getenv('DB_USER', 'ailog_analyzer_user_d1'),
    'password': os.getenv('DB_PASSWORD')  # Z .env souboru
}
conn = psycopg2.connect(**DB_CONFIG)
cursor = conn.cursor()

# Norm√°ln√≠ operace (INSERT/SELECT/UPDATE/DELETE)
cursor.execute("SELECT * FROM ailog_peak.peak_statistics LIMIT 10")
```

**2. DDL OPERACE (CREATE TABLE, ALTER, GRANT):**
```python
from dotenv import load_dotenv
import psycopg2
import os

load_dotenv()

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'P050TD01.DEV.KB.CZ'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'ailog_analyzer'),
    'user': os.getenv('DB_DDL_USER', 'ailog_analyzer_ddl_user_d1'),  # DDL user!
    'password': os.getenv('DB_DDL_PASSWORD')  # Z .env souboru
}
conn = psycopg2.connect(**DB_CONFIG)
cursor = conn.cursor()

# ‚ö†Ô∏è POVINN√â: SET ROLE p≈ôed DDL operacemi
cursor.execute("SET ROLE role_ailog_analyzer_ddl;")
print("‚úÖ DDL role set")

# Nyn√≠ m≈Ø≈æe≈° dƒõlat DDL operace
cursor.execute("CREATE SCHEMA IF NOT EXISTS ailog_peak;")
cursor.execute("CREATE TABLE IF NOT EXISTS ailog_peak.peak_statistics (...);")
cursor.execute("GRANT SELECT ON ailog_peak.peak_statistics TO ailog_analyzer_user_d1;")
conn.commit()
```

**V≈ΩDY kontroluj NEJD≈ò√çV:**
1. Existuje .env soubor? `ls -la .env`
2. M√° DB_PASSWORD? `grep DB_PASSWORD .env` (bez zobrazen√≠ hodnoty)
3. Volej `load_dotenv()` P≈òED `psycopg2.connect()`!
4. Pro DDL: pou≈æij DB_DDL_USER + SET ROLE!

**V≈°echny DB skripty pou≈æ√≠vaj√≠ tento p≈ô√≠stup:**
- **Data skripty:** verify_peak_data.py, ingest_from_log.py, clear_peak_db.py
- **DDL skripty:** setup_peak_db.py, grant_permissions.py, init_peak_statistics_db.py

### Elasticsearch (FIXED VALUES)
```bash
# Stejn√© pro v≈°echny - NEMƒöNIT!
ES_URL=https://elasticsearch-test.kb.cz:9500
ES_VERIFY_CERTS=false

# Specifick√© pro va≈°i aplikaci:
ES_INDEX=cluster-app_<vase_aplikace>-*  # nap≈ô. pcb-*, pca-*, relay-*
ES_USER=XX_<VASE_APP>_ES_READ            # z SMAX
ES_PASSWORD=<z_emailu>                   # z SMAX
```

### Environment Setup
```bash
# 1. Zkop√≠rovat template
cp .env.example .env

# 2. Vyplnit sv√© hodnoty
nano .env

# 3. Spustit skripty (naƒçtou automaticky)
python scripts/analyze_period.py ...
```

**See:** [ENV_SETUP.md](ENV_SETUP.md) pro detaily

---

## üõ†Ô∏è WORKFLOW: Jak Pokraƒçovat

### 1. START NOV√â SESSION
```bash
cd /home/jvsete/git/sas/ai-log-analyzer

# Zkontroluj git status
git status
git log --oneline -5

# P≈ôeƒçti posledn√≠ progress
cat working_progress.md | tail -100
```

### 2. SPUS≈§ SCRIPT Z `scripts/` SLO≈ΩKY
```bash
# V≈°echny scripty jsou teƒè v scripts/
cd scripts/

# Nap≈ô√≠klad: collect data
python collect_peak_detailed.py --from 2025-12-16T00:00:00Z --to 2025-12-17T00:00:00Z

# Nebo: verify data
python verify_peak_data.py

# Nebo: export to CSV
python export_peak_statistics.py --from 2025-12-01 --to 2025-12-16

# HELP - co dƒõl√° ka≈æd√Ω script?
cat INDEX.md
```

### 3. COMMIT ZMƒöNY
```bash
git add -A
git commit -m "Phase 5: [brief description]"
git push origin feature/ai-log-analyzer-v2
```

---

## üìö ACTIVE DOCUMENTATION

### PRIMARY (aktu√°ln√≠, pou≈æ√≠vej):
| Soubor | Obsah | Kdy |
|--------|-------|-----|
| **working_progress.md** | Session log + TODO | Ka≈æd√Ω den |
| **scripts/INDEX.md** | Script reference | Spou≈°tƒõn√≠ scripts |
| **README.md** | Project overview | First time |
| **CONTEXT_RETRIEVAL_PROTOCOL.md** | Tenhle soubor | Kontext p≈ôenosu |
| **HOW_TO_USE.md** | User guide | Development |

### ARCHIVED (zastaral√©, ignoruj):
- `_archive_md/COMPLETED_LOG.md` - Star√Ω session log
- `_archive_md/DEPLOYMENT.md` - Zastaral√© deployment noty
- `_archive_md/PHASE_ROADMAP.md` - Star√Ω roadmap
- ‚Üí Viz `_archive_md/` pro √∫pln√Ω seznam

---

## üéØ PHASE 5 WORKFLOW - Co Dƒõlat Dnes

### Krok 1: Export Data (if needed)
```bash
cd scripts/
python export_peak_statistics.py --from 2025-12-01 --to 2025-12-16
# Vytvo≈ô√≠: peak_statistics_export_YYYYMMDD_HHMMSS.csv
```

### Krok 2: Verify Current Data
```bash
python verify_peak_data.py
# Kontroluje: duplicates, NaN values, date ranges
```

### Krok 3: PHASE 5B - Load Production Data
```bash
# Step 1: DELETE testovac√≠ data
psql -h P050TD01.DEV.KB.CZ -U ailog_analyzer_user_d1 -d ailog_analyzer
DELETE FROM ailog_peak.peak_statistics WHERE 1=1;

# Step 2: Prepare chybƒõj√≠c√≠ data (2025-12-02 a≈æ 2025-12-15)
# Ke ka≈æd√©mu dni:
python collect_peak_detailed.py --from "2025-12-02T00:00:00Z" --to "2025-12-03T00:00:00Z"
# Output: /tmp/peak_data_2025_12_02.txt

# Step 3: Load do DB (skript ingest_peak_statistics.py TBD)
# (zat√≠m ruƒçnƒõ, nebo skript kter√Ω existuje)

# Step 4: Validate
python verify_peak_data.py
```

### Krok 4: Commit & Update
```bash
cd /home/jvsete/git/sas/ai-log-analyzer
git add -A
git commit -m "Phase 5B: Production data ingestion (2025-12-01 to 2025-12-15)"
git push
```

---

## ‚úÖ CHECKLIST - N√°vrat k Projektu

Standardn√≠ postup kdy≈æ zaƒç√≠n√°≈°:

- [ ] Zkontroluj posledn√≠ commit: `git log --oneline -3`
- [ ] P≈ôeƒçti progress: `cat working_progress.md | tail -50`
- [ ] Zkontroluj branchy: `git branch -v`
- [ ] Aktualizuj si kontext: `cat CONTEXT_RETRIEVAL_PROTOCOL.md`
- [ ] Spus≈• script z `scripts/` (viz `scripts/INDEX.md`)
- [ ] Loguj progress do `working_progress.md`
- [ ] Commit + push

---

## üìä SCRIPTS QUICK REFERENCE

| Script | Typ | Popis | Last Run |
|--------|-----|-------|----------|
| **collect_peak_detailed.py** | ‚≠ê Core | Sb√≠r√° peak data z ES | 2025-12-15 |
| **fetch_unlimited.py** | Util | ES query helper | N/A |
| **analyze_period.py** | Util | Full pipeline | 2025-12-16 |
| **export_peak_statistics.py** | Export | Data ‚Üí CSV | 2025-12-16 |
| **verify_peak_data.py** | Validation | DB checks | Pending |
| **init_peak_statistics_db.py** | Setup (1x) | Create tables | 2025-12-12 |

‚Üí **FULL DETAILS:** `scripts/INDEX.md`

---

## üì¶ ARCHIVE & CLEANUP (2025-12-16)

```
‚úÖ DONE:
- Workspace cleanup: 6 archiv slo≈æek
- Scripts reorganizace: do scripts/ s INDEX.md
- MD soubory: archivov√°ny do _archive_md/
- k8s/: archivov√°n (zastaral√© nasazen√≠)

üìä SIZE REDUCTION:
- P≈Øvodnƒõ: 618MB
- Nyn√≠: ~404MB (200MB cleanup)
- Root: 4 MD + config files (ƒçist√Ω!)
```

---

**Version:** 2.1 (Updated 2025-12-16 11:00 UTC)  
**Status:** ‚úÖ Phase 4 Complete | üîÑ Phase 5 - Peak Detection  
**Maintainer:** jvsete + AI Assistant  
**Branch:** `feature/ai-log-analyzer-v2` (k8s-infra-apps-nprod repo)

