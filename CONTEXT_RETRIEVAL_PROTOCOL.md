# CONTEXT RETRIEVAL PROTOCOL
## AI Log Analyzer - Quick Reference

**Verze:** 2.2  
**Last Update:** 2025-12-17  
**ÃšÄel:** RychlÃ½ pÅ™ehled pro pokraÄovÃ¡nÃ­ v prÃ¡ci

---

## ğŸ“‹ CO TO JE

**AI Log Analyzer** - AutomatickÃ¡ analÃ½za errorÅ¯ z Elasticsearch s AI doporuÄenÃ­mi

- **Tech Stack:** Python + FastAPI + PostgreSQL + Elasticsearch + Ollama (optional)
- **Deployment:** Kubernetes (ArgoCD) + Harbor registry  
- **Current Phase:** Phase 5 (Peak Detection Baseline)

---

## ğŸ¯ AKTUÃLNÃ STAV (2025-12-16 11:00 UTC - Phase 5 IN PROGRESS)

### âœ… HOTOVO (Phase 4 + 5 setup)
1. **Docker Image** âœ…
   - Tag: `v0.4.0-docker-verified` + `latest`
   - Registry: `dockerhub.kb.cz/pccm-sq016/ai-log-analyzer`

2. **Database Schema** âœ…
   - PostgreSQL: P050TD01.DEV.KB.CZ:5432/ailog_analyzer
   - Schema: ailog_peak

3. **Phase 5: Peak Data Collection** âœ…
   - âœ… collect_peak_detailed.py: 2025-12-01 (230K errors, ready for load)
   - âœ… Scripts reorganized do `scripts/` s `scripts/INDEX.md`
   - âœ… Workspace cleanup (6 archivÅ¯)
   - âš ï¸ DB: testovacÃ­ data (2025-12-05) - BUDOU SMAZANA

### ğŸ”„ V PROCESU (Phase 5B - Priority: DATA INGESTION)

**Production Data Status:**
```
2025-12-01: âœ… 230,146 errors (4 namespaces: pcb-*)
            Lokace: /tmp/peak_data_2025_12_01.txt
            âš ï¸ ChybÃ­: pca-dev, pca-sit

2025-12-02 aÅ¾ 2025-12-15: âŒ CHYBÃ - Nutno stÃ¡hnout
2025-12-16: â³ TODAY - JeÅ¡tÄ› se sbÃ­rÃ¡
```

**DB Current State:**
```
âŒ TestovacÃ­ data (budou smazana):
   - 2,623 rows z 2025-12-05
   - 6 namespaces (pca-* + pcb-*)
   - Status: TO DELETE
```

**Next 5 Steps (PRIORITY ORDER):**
1. [ ] **Smazat** testovacÃ­ data z DB
2. [ ] **StÃ¡hnout** chybÄ›jÃ­cÃ­ data 2025-12-02 aÅ¾ 2025-12-15
3. [ ] **OvÄ›Å™it** formÃ¡t dat z 2025-12-01
4. [ ] **NataÅ¾** vÅ¡ech dat do DB (s smoothingem)
5. [ ] **Validovat** kompletnÃ­ range 2025-12-01 aÅ¾ 2025-12-15

### ğŸ“‹ NEXT (Priority Order)
1. â­ï¸ Load data into DB - Phase 5B (THIS PRIORITY!)
2. â­ï¸ Create ingest_peak_statistics.py
3. â­ï¸ Deploy to K8s cluster (Phase 6)
4. â­ï¸ Cluster automate (Phase 7)

---

## ğŸ“ WORKSPACE STRUKTURA (2025-12-16)

```
ai-log-analyzer/
â”œâ”€â”€ ğŸ“„ README.md                      # â­ Main documentation
â”œâ”€â”€ ğŸ“„ CONTEXT_RETRIEVAL_PROTOCOL.md  # â­ This file - quick context
â”œâ”€â”€ ğŸ“„ working_progress.md            # â­ Session log + tasks
â”œâ”€â”€ ğŸ“„ HOW_TO_USE.md                  # â­ User guide + examples
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                       # ALL PRODUCTION SCRIPTS
â”‚   â”œâ”€â”€ INDEX.md                      # ğŸ“‹ Script reference (START HERE!)
â”‚   â”œâ”€â”€ collect_peak_detailed.py      # â­ CORE - Peak data collector
â”‚   â”œâ”€â”€ fetch_unlimited.py            # Elasticsearch fetcher
â”‚   â”œâ”€â”€ analyze_period.py             # Orchestrator
â”‚   â”œâ”€â”€ export_peak_statistics.py     # CSV export
â”‚   â”œâ”€â”€ verify_peak_data.py           # Data validation
â”‚   â”œâ”€â”€ init_peak_statistics_db.py    # DB init (1x)
â”‚   â”œâ”€â”€ setup_peak_db.py              # DB setup (1x)
â”‚   â”œâ”€â”€ grant_permissions.py          # DB perms (1x)
â”‚   â”œâ”€â”€ create_known_issues_registry.py # Known issues
â”‚   â””â”€â”€ workflow_manager.sh           # Shell wrapper
â”‚
â”œâ”€â”€ ğŸ“‚ app/                           # FastAPI application
â”‚   â”œâ”€â”€ main.py                       # Entry point
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ ğŸ“‚ alembic/                       # Database migrations
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ env.py
â”‚
â”œâ”€â”€ ğŸ“‚ _archive_md/                   # OLD Documentation (ignore)
â”‚   â”œâ”€â”€ COMPLETED_LOG.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â”œâ”€â”€ HARBOR_DEPLOYMENT_GUIDE.md
â”‚   â”œâ”€â”€ KNOWN_ISSUES_DESIGN.md
â”‚   â”œâ”€â”€ PHASE_ROADMAP.md
â”‚   â””â”€â”€ README_SCRIPTS.md
â”‚
â”œâ”€â”€ ğŸ“‚ _archive_scripts/              # OLD Scripts from Phase 1-3
â”‚   â””â”€â”€ (19 zastaralÃ½ch skriptÅ¯)
â”‚
â”œâ”€â”€ ğŸ“‚ _archive_old/                  # OLD Folders (cleanup 2025-12-16)
â”‚   â”œâ”€â”€ k8s/                          # ZastaralÃ© manifesty
â”‚   â”œâ”€â”€ copilot-chat-backups/         # Chat backupy
â”‚   â”œâ”€â”€ updates/                      # StarÃ© session noty
â”‚   â”œâ”€â”€ .backup_2025-11-18/           # StarÃ½ backup
â”‚   â””â”€â”€ tests/                        # PrÃ¡zdnÃ½ test folder
â”‚
â”œâ”€â”€ ğŸ³ Dockerfile                     # Current image build
â”œâ”€â”€ ğŸ“¦ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ“‹ docker-compose.yml             # Dev environment
â”œâ”€â”€ ğŸ”‘ .env                           # Configuration (git-ignored)
â”œâ”€â”€ alembic.ini                       # DB migration config
â”œâ”€â”€ pyproject.toml                    # Python project config
â””â”€â”€ .gitignore                        # Git ignore rules
```

---

## ğŸ”‘ KLÃÄŒOVÃ‰ INFORMACE

### Database Connection
```
Host: P050TD01.DEV.KB.CZ
Port: 5432
Database: ailog_analyzer
Schema: ailog_peak (tables: peak_statistics, known_errors, etc.)
User: ailog_analyzer_user_d1 (via Cyberark DAP_PCB safe)
```

### Elasticsearch (FIXED VALUES)
```bash
# StejnÃ© pro vÅ¡echny - NEMÄšNIT!
ES_URL=https://elasticsearch-test.kb.cz:9500
ES_VERIFY_CERTS=false

# SpecifickÃ© pro vaÅ¡i aplikaci:
ES_INDEX=cluster-app_<vase_aplikace>-*  # napÅ™. pcb-*, pca-*, relay-*
ES_USER=XX_<VASE_APP>_ES_READ            # z SMAX
ES_PASSWORD=<z_emailu>                   # z SMAX
```

### Environment Setup
```bash
# 1. ZkopÃ­rovat template
cp .env.example .env

# 2. Vyplnit svÃ© hodnoty
nano .env

# 3. Spustit skripty (naÄtou automaticky)
python scripts/analyze_period.py ...
```

**See:** [ENV_SETUP.md](ENV_SETUP.md) pro detaily

---

## ğŸ› ï¸ WORKFLOW: Jak PokraÄovat

### 1. START NOVÃ‰ SESSION
```bash
cd /home/jvsete/git/sas/ai-log-analyzer

# Zkontroluj git status
git status
git log --oneline -5

# PÅ™eÄti poslednÃ­ progress
cat working_progress.md | tail -100
```

### 2. SPUSÅ¤ SCRIPT Z `scripts/` SLOÅ½KY
```bash
# VÅ¡echny scripty jsou teÄ v scripts/
cd scripts/

# NapÅ™Ã­klad: collect data
python collect_peak_detailed.py --from 2025-12-16T00:00:00Z --to 2025-12-17T00:00:00Z

# Nebo: verify data
python verify_peak_data.py

# Nebo: export to CSV
python export_peak_statistics.py --from 2025-12-01 --to 2025-12-16

# HELP - co dÄ›lÃ¡ kaÅ¾dÃ½ script?
cat INDEX.md
```

### 3. COMMIT ZMÄšNY
```bash
git add -A
git commit -m "Phase 5: [brief description]"
git push origin feature/ai-log-analyzer-v2
```

---

## ğŸ“š ACTIVE DOCUMENTATION

### PRIMARY (aktuÃ¡lnÃ­, pouÅ¾Ã­vej):
| Soubor | Obsah | Kdy |
|--------|-------|-----|
| **working_progress.md** | Session log + TODO | KaÅ¾dÃ½ den |
| **scripts/INDEX.md** | Script reference | SpouÅ¡tÄ›nÃ­ scripts |
| **README.md** | Project overview | First time |
| **CONTEXT_RETRIEVAL_PROTOCOL.md** | Tenhle soubor | Kontext pÅ™enosu |
| **HOW_TO_USE.md** | User guide | Development |

### ARCHIVED (zastaralÃ©, ignoruj):
- `_archive_md/COMPLETED_LOG.md` - StarÃ½ session log
- `_archive_md/DEPLOYMENT.md` - ZastaralÃ© deployment noty
- `_archive_md/PHASE_ROADMAP.md` - StarÃ½ roadmap
- â†’ Viz `_archive_md/` pro ÃºplnÃ½ seznam

---

## ğŸ¯ PHASE 5 WORKFLOW - Co DÄ›lat Dnes

### Krok 1: Export Data (if needed)
```bash
cd scripts/
python export_peak_statistics.py --from 2025-12-01 --to 2025-12-16
# VytvoÅ™Ã­: peak_statistics_export_YYYYMMDD_HHMMSS.csv
```

### Krok 2: Verify Current Data
```bash
python verify_peak_data.py
# Kontroluje: duplicates, NaN values, date ranges
```

### Krok 3: PHASE 5B - Load Production Data
```bash
# Step 1: DELETE testovacÃ­ data
psql -h P050TD01.DEV.KB.CZ -U ailog_analyzer_user_d1 -d ailog_analyzer
DELETE FROM ailog_peak.peak_statistics WHERE 1=1;

# Step 2: Prepare chybÄ›jÃ­cÃ­ data (2025-12-02 aÅ¾ 2025-12-15)
# Ke kaÅ¾dÃ©mu dni:
python collect_peak_detailed.py --from "2025-12-02T00:00:00Z" --to "2025-12-03T00:00:00Z"
# Output: /tmp/peak_data_2025_12_02.txt

# Step 3: Load do DB (skript ingest_peak_statistics.py TBD)
# (zatÃ­m ruÄnÄ›, nebo skript kterÃ½ existuje)

# Step 4: Validate
python verify_peak_data.py
```

### Krok 4: Commit & Update
```bash
cd /home/jvsete/git/sas/ai-log-analyzer
git add -A
git commit -m "Phase 5B: Production data ingestion (2025-12-01 to 2025-12-15)"
git push
```

---

## âœ… CHECKLIST - NÃ¡vrat k Projektu

StandardnÃ­ postup kdyÅ¾ zaÄÃ­nÃ¡Å¡:

- [ ] Zkontroluj poslednÃ­ commit: `git log --oneline -3`
- [ ] PÅ™eÄti progress: `cat working_progress.md | tail -50`
- [ ] Zkontroluj branchy: `git branch -v`
- [ ] Aktualizuj si kontext: `cat CONTEXT_RETRIEVAL_PROTOCOL.md`
- [ ] SpusÅ¥ script z `scripts/` (viz `scripts/INDEX.md`)
- [ ] Loguj progress do `working_progress.md`
- [ ] Commit + push

---

## ğŸ“Š SCRIPTS QUICK REFERENCE

| Script | Typ | Popis | Last Run |
|--------|-----|-------|----------|
| **collect_peak_detailed.py** | â­ Core | SbÃ­rÃ¡ peak data z ES | 2025-12-15 |
| **fetch_unlimited.py** | Util | ES query helper | N/A |
| **analyze_period.py** | Util | Full pipeline | 2025-12-16 |
| **export_peak_statistics.py** | Export | Data â†’ CSV | 2025-12-16 |
| **verify_peak_data.py** | Validation | DB checks | Pending |
| **init_peak_statistics_db.py** | Setup (1x) | Create tables | 2025-12-12 |

â†’ **FULL DETAILS:** `scripts/INDEX.md`

---

## ğŸ“¦ ARCHIVE & CLEANUP (2025-12-16)

```
âœ… DONE:
- Workspace cleanup: 6 archiv sloÅ¾ek
- Scripts reorganizace: do scripts/ s INDEX.md
- MD soubory: archivovÃ¡ny do _archive_md/
- k8s/: archivovÃ¡n (zastaralÃ© nasazenÃ­)

ğŸ“Š SIZE REDUCTION:
- PÅ¯vodnÄ›: 618MB
- NynÃ­: ~404MB (200MB cleanup)
- Root: 4 MD + config files (ÄistÃ½!)
```

---

**Version:** 2.1 (Updated 2025-12-16 11:00 UTC)  
**Status:** âœ… Phase 4 Complete | ğŸ”„ Phase 5 - Peak Detection  
**Maintainer:** jvsete + AI Assistant  
**Branch:** `feature/ai-log-analyzer-v2` (k8s-infra-apps-nprod repo)

