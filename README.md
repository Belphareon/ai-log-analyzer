# AI Log Analyzer V4

**DeterministickÃ½ incident detektor pro Elasticsearch logy**

## ðŸ“ Struktura projektu

```
ai-log-analyzer/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ v4/                    # Pipeline V4 (hlavnÃ­)
â”‚   â”‚   â”œâ”€â”€ incident.py        # Incident Object
â”‚   â”‚   â”œâ”€â”€ phase_a_parse.py   # Parse & Normalize
â”‚   â”‚   â”œâ”€â”€ phase_b_measure.py # Measure (EWMA, MAD)
â”‚   â”‚   â”œâ”€â”€ phase_c_detect.py  # Detect (boolean flags)
â”‚   â”‚   â”œâ”€â”€ phase_d_score.py   # Score (vÃ¡hovÃ¡ funkce)
â”‚   â”‚   â”œâ”€â”€ phase_e_classify.py# Classify (taxonomy)
â”‚   â”‚   â”œâ”€â”€ phase_f_report.py  # Report (render)
â”‚   â”‚   â””â”€â”€ pipeline_v4.py     # Main orchestrator
â”‚   â”œâ”€â”€ core/                  # Core komponenty
â”‚   â”‚   â”œâ”€â”€ fetch_unlimited.py # ES fetcher (search_after)
â”‚   â”‚   â”œâ”€â”€ collect_peak_detailed.py
â”‚   â”‚   â”œâ”€â”€ peak_detection_v3.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/                 # Utility skripty
â”‚   â””â”€â”€ migrations/            # SQL migrace
â”œâ”€â”€ k8s/                       # Kubernetes manifests
â”œâ”€â”€ config/                    # Konfigurace
â”œâ”€â”€ docs/                      # Dokumentace
â”œâ”€â”€ data/                      # Data adresÃ¡Å™e
â”‚   â”œâ”€â”€ batches/              # DoÄasnÃ© batch soubory
â”‚   â”œâ”€â”€ reports/              # GenerovanÃ© reporty
â”‚   â””â”€â”€ snapshots/            # Snapshoty pro replay
â”œâ”€â”€ run_init.sh               # SpustÃ­ INIT fÃ¡zi
â”œâ”€â”€ run_regular.sh            # SpustÃ­ REGULAR fÃ¡zi
â”œâ”€â”€ run_backfill.sh           # Backfill poslednÃ­ch N dnÃ­
â””â”€â”€ requirements.txt
```

## ðŸš€ Quick Start

### 1. NastavenÃ­ prostÅ™edÃ­

```bash
# VytvoÅ™ .env soubor
cp config/.env.example .env

# Uprav .env s tvÃ½mi credentials
vim .env

# Instalace zÃ¡vislostÃ­
pip install -r requirements.txt
```

### 2. DatabÃ¡ze - migrace

```bash
# PÅ™ipojenÃ­ k DB
export PGPASSWORD=$DB_PASSWORD

# SpusÅ¥ migrace
psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -f scripts/migrations/000_create_base_tables.sql
psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -f scripts/migrations/001_create_peak_thresholds.sql
psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -f scripts/migrations/002_create_enhanced_analysis_tables.sql
```

### 3. INIT FÃ¡ze (jednorÃ¡zovÄ›)

SbÃ­rÃ¡ baseline data za 21+ dnÃ­ BEZ peak detection:

```bash
# SbÄ›r dat za poslednÃ­ 3 tÃ½dny
./run_init.sh --days 21

# Nebo konkrÃ©tnÃ­ obdobÃ­
./run_init.sh --from "2025-12-01T00:00:00Z" --to "2025-12-21T23:59:59Z"

# Dry run (bez zÃ¡pisu do DB)
./run_init.sh --days 21 --dry-run
```

### 4. VÃ½poÄet thresholds (po INIT)

```bash
python scripts/core/calculate_peak_thresholds.py
```

### 5. Backfill (zpracovÃ¡nÃ­ historickÃ½ch dat)

Zpracuje poslednÃ­ch N dnÃ­ S peak detection:

```bash
# Backfill poslednÃ­ch 14 dnÃ­
./run_backfill.sh --days 14

# S uloÅ¾enÃ­m reportÅ¯
./run_backfill.sh --days 14 --output data/reports/
```

### 6. REGULAR FÃ¡ze (cron kaÅ¾dÃ½ch 15 minut)

```bash
# ManuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
./run_regular.sh

# S uloÅ¾enÃ­m reportu
./run_regular.sh --output data/reports/

# Quiet mode (pro cron)
./run_regular.sh --quiet
```

## â° Cron Setup

### Linux crontab

```cron
# KaÅ¾dÃ½ch 15 minut
*/15 * * * * cd /path/to/ai-log-analyzer && ./run_regular.sh --quiet >> /var/log/ailog/cron.log 2>&1
```

### Kubernetes CronJob

```bash
kubectl apply -f k8s/cronjob.yaml
```

## ðŸ“Š Pipeline V4 Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE A â”‚â”€â”€â”€â”€â–¶â”‚ PHASE B â”‚â”€â”€â”€â”€â–¶â”‚ PHASE C â”‚â”€â”€â”€â”€â–¶â”‚ PHASE D â”‚â”€â”€â”€â”€â–¶â”‚ PHASE E â”‚â”€â”€â”€â”€â–¶â”‚ PHASE F â”‚
â”‚  PARSE  â”‚     â”‚ MEASURE â”‚     â”‚ DETECT  â”‚     â”‚  SCORE  â”‚     â”‚CLASSIFY â”‚     â”‚ REPORT  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚               â”‚               â”‚               â”‚
     â–¼               â–¼               â–¼               â–¼               â–¼               â–¼
 fingerprint    EWMA/MAD        boolean         score          category         JSON/MD
 normalized     baseline        flags           0-100          taxonomy         console
```

| FÃ¡ze | Vstup | VÃ½stup | Popis |
|------|-------|--------|-------|
| A | raw errors | normalized records | Normalizace, fingerprint |
| B | records | measurements | EWMA baseline, MAD, trend |
| C | measurements | flags + evidence | is_spike, is_new, is_burst |
| D | flags | score | DeterministickÃ¡ vÃ¡hovÃ¡ funkce |
| E | message | category | Taxonomy klasifikace |
| F | incidents | report | JSON, MD, console |

## ðŸ”§ Konfigurace

### Environment variables (.env)

```bash
# Elasticsearch
ES_HOST=https://elasticsearch.example.com:9500
ES_USER=your_user
ES_PASSWORD=your_password
ES_INDEX=cluster-app_pcb-*

# PostgreSQL
DB_HOST=postgres.example.com
DB_PORT=5432
DB_NAME=ailog_analyzer
DB_USER=ailog_user
DB_PASSWORD=your_password

# Pipeline
SPIKE_THRESHOLD=3.0
EWMA_ALPHA=0.3

# Notifications (optional)
TEAMS_WEBHOOK_URL=https://outlook.office.com/webhook/...
```

### config/namespaces.yaml

```yaml
namespaces:
  - pcb-dev-01-app
  - pcb-sit-01-app
  - pcb-uat-01-app
  - pcb-prd-01-app
```

## ðŸ“ˆ VÃ½stupy

### Incident Object (JSON)

```json
{
  "id": "inc-20260120-001",
  "fingerprint": "abc123def456",
  "score": 72,
  "severity": "high",
  "category": "network",
  "flags": {
    "spike": true,
    "new": false,
    "cross_namespace": true
  },
  "evidence": [
    {
      "rule": "spike_ewma",
      "baseline": 10.5,
      "current": 52.0,
      "threshold": 3.0
    }
  ]
}
```

### Replay (regression testing)

```bash
# UloÅ¾enÃ­ snapshotu
./run_regular.sh --output data/snapshots/

# PozdÄ›jÅ¡Ã­ porovnÃ¡nÃ­
python scripts/v4/pipeline_v4.py data/batches/ --replay data/snapshots/summary_20260120.json
```

## ðŸ“š Dokumentace

- [Pipeline V4 Architecture](docs/PIPELINE_V4_ARCHITECTURE.md)
- [Incident Object Reference](docs/INCIDENT_OBJECT.md)
- [Database Schema](docs/DATABASE_SCHEMA.md)
- [Troubleshooting](docs/TROUBLESHOOTING.md)

## ðŸ”’ PoÅ¾adavky

- Python 3.10+
- PostgreSQL 13+
- Elasticsearch 7.x/8.x
- Kubernetes 1.24+ (pro K8s deployment)

## ðŸ“¦ ZÃ¡vislosti

```
psycopg2-binary>=2.9.0
python-dotenv>=1.0.0
requests>=2.28.0
PyYAML>=6.0
```

---

**Verze:** 4.0 | **Datum:** 2026-01-20
