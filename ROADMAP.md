# ğŸ—ºï¸ AI Log Analyzer - Project Roadmap

**Last Updated:** 2026-01-19

---

## ğŸ“Š Project Phases

### âœ… Phase 1: EXTENDED INIT (Complete - 2026-01-19)

**Objective:** Load historical baseline data + early January data into database

**Tasks:**
- âœ… Extract all 31 days of December 2025
- âœ… Ingest 13,482 December data rows
- âœ… Ingest January 1-2 from batch files (945 rows)
- âœ… **EXTENDED**: Ingest January 3-6 12:00 from batch files (1,627 rows)
- âœ… Fill missing windows with zeros â†’ 42,790 rows total
- âœ… Backup peak_raw_data: 9.9 MB
- âœ… Backup aggregation_data: 1.8 MB

**Output:**
```
peak_raw_data:      42,790 rows (complete grid: 37 days Ã— 12 namespaces Ã— 96 windows)
aggregation_data:    8,064 rows (baseline pattern for reference)
```

**Key Data Points:**
- December 1-31, 2025: All 31 individual day files
- January 1-2, 2026: From batch files
- January 3-6 (until 12:00), 2026: From batch files (all 12 namespaces now complete)
- Reason for extension: ES data only complete from Jan 6 12:00 onwards

---

### ğŸ”„ Phase 2: REGULAR (Next - Ready to Start)

**Objective:** Process January 7+ 2026 data with real-time peak detection

**Tasks:**
- â³ Prepare January 7-31 2026 source data from Elasticsearch
- â³ Ingest daily data from January 7 onwards
- â³ Compare against aggregation baseline
- â³ Detect and categorize peaks in `peak_investigation`
- â³ Apply dynamic thresholds from `values.yaml`
- â³ Track known patterns and error distribution

**Expected Output:**
```
peak_raw_data:       Growing (30-day rolling retention)
aggregation_data:    Updated daily (rolling 7-day pattern)
peak_investigation:  Grows with detected anomalies
error_patterns:      Learned patterns from deviations
```

**Command:**
```bash
python3 scripts/run_pipeline.py --from "2026-01-07T00:00:00Z" --to "2026-01-31T23:59:59Z"
```

---

### ğŸš€ Phase 3: Analysis & Optimization (Pending)

**Objective:** Deep analysis of detected peaks and pattern learning

**Tasks:**
- â³ Analyze peak frequency per namespace
- â³ Identify repeating patterns
- â³ Auto-adjust thresholds based on learning
- â³ Generate summary reports
- â³ Create dashboards (if UI component added)

---

## ğŸ¯ Current Status Matrix

| Component | Status | Version | Last Updated |
|-----------|--------|---------|--------------|
| **INIT Phase - December** | âœ… Complete | v1.0 | 2026-01-19 |
| **INIT Phase - January 1-6** | âœ… Complete | v1.0 | 2026-01-19 |
| **Data Ingestion** | âœ… Complete | v2.0 | 2026-01-19 |
| **Fill Missing Windows** | âœ… Complete | v2.0 | 2026-01-19 |
| **Database Backup** | âœ… Complete | v1.0 | 2026-01-19 |
| **Peak Detection** | âœ… Ready | v1.0 | 2025-12-28 |
| **Baseline Recalc** | â³ Pending | v1.0 | - |
| **REGULAR Phase** | â³ Ready | v1.0 | 2026-01-19 |
| **LLM Integration** | â³ Pending | v0.0 | - |
| **UI/Dashboard** | â³ Pending | v0.0 | - |

---

## ğŸ“ File Structure

```
ai-log-analyzer/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ batch_ingest.sh                    # INIT/REGULAR orchestrator âœ…
â”‚   â”œâ”€â”€ ingest_from_log_v2.py             # Data ingestion engine âœ…
â”‚   â”œâ”€â”€ fill_missing_windows_fast.py      # Grid completion âœ…
â”‚   â”œâ”€â”€ calculate_aggregation_baseline.py # Baseline computation âœ…
â”‚   â”œâ”€â”€ run_pipeline.py                    # Full pipeline runner
â”‚   â””â”€â”€ INDEX.md                           # Scripts documentation
â”œâ”€â”€ values.yaml                             # Dynamic threshold config âœ…
â”œâ”€â”€ .env                                    # Database credentials âœ…
â”œâ”€â”€ _backups/                               # Database backups âœ…
â”‚   â””â”€â”€ ailog_peak_*_20260119_092834.sql  # INIT backup
â”œâ”€â”€ ROADMAP.md                              # This file
â”œâ”€â”€ README.md                               # Project overview
â”œâ”€â”€ CONTEXT.md                              # Session context âœ…
â””â”€â”€ STATUS.md                               # Detailed status
```

---

## ğŸ”§ Key Technical Decisions

1. **Database Schema**: PostgreSQL with `ailog_peak` schema containing 6 tables
2. **Data Format**: Pipe-delimited with specific field ordering
3. **Baseline Approach**: Aggregation per day-of-week (7-day pattern)
4. **Peak Detection**: Ratio-based (threshold multipliers in values.yaml)
5. **Retention Policy**: 30-day rolling window for raw data

---

## âš ï¸ Known Issues & Workarounds

| Issue | Status | Notes |
|-------|--------|-------|
| Batch files contain 3 days due to timezone fixes | âœ… Resolved | Pattern extraction implemented |
| Day 15 missing from initial conversion | âœ… Resolved | Manually extracted from batch file |
| 735 duplicate rows in final dataset | âš ï¸ Acceptable | Identified as non-critical duplicates |
| LDAP authentication with psql | âš ï¸ Workaround | Using Python psycopg2 instead |

---

## ğŸ“ˆ Metrics & Goals

**Data Coverage:**
- âœ… December 2025: 100% (31/31 days)
- âœ… January 1-6 (12:00) 2026: 100% (3.5/3.5 days)
- â³ January 7-31 2026: 0/25 days (REGULAR phase)

**Database Health:**
- âœ… peak_raw_data: 42,790 rows (100% coverage for Dec + Jan 1-6)
- âœ… aggregation_data: 8,064 rows (baseline ready)
- âœ… peak_investigation: 0 rows (ready for REGULAR phase)

**Performance Targets:**
- Daily ingestion: < 5 minutes per day
- Fill missing windows: < 30 seconds
- Baseline calculation: < 1 minute
- Peak detection: < 10 minutes per 1000 events
- Backup creation: < 2 minutes

---

## ğŸš€ Next Immediate Steps

1. **Recalculate baseline** (if needed): `python3 scripts/calculate_aggregation_baseline.py`
2. **Start REGULAR phase**: `python3 scripts/run_pipeline.py --from "2026-01-07T00:00:00Z"`
3. **Monitor peaks**: Check `peak_investigation` table for anomalies
4. **Review thresholds**: Adjust `values.yaml` based on early results
5. **Plan Phase 3**: Analysis and dashboard components

---

## ğŸ’¾ Backup & Recovery

**Current Backup:** `_backups/ailog_peak_*_20260119_1303*.sql`
- peak_raw_data: 9.9M (42,790 rows)
- aggregation_data: 1.8M (8,064 rows)
- Total: 11.7M

**Restore Command (using Python script):**
```bash
cd /home/jvsete/git/sas/ai-log-analyzer
# Restore individual tables by replaying INSERT statements from backup files
python3 << 'PYEOF'
import psycopg2
# Read and execute INSERT statements from backup files
PYEOF
```

---

## ğŸ“ Contact & Resources

- **Project Location**: `/home/jvsete/git/sas/ai-log-analyzer`
- **Data Location**: `/tmp/ai-log-data/` (batch files: peak_2026_01_*_TS.txt)
- **Database**: `P050TD01.DEV.KB.CZ:5432/ailog_analyzer` (schema: `ailog_peak`)
- **Documentation**: See [README.md](README.md) and [scripts/INDEX.md](scripts/INDEX.md)
