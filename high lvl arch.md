â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ELASTICSEARCH                         â”‚
â”‚                   (vÅ¡echny errory)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  collect_peak_detailed.py   â”‚
         â”‚  (agregace 15-min windows)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ingest_from_log_v2.py     â”‚
         â”‚   (peak detection)          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PEAK?  â”‚    â”‚ NO PEAK  â”‚   â”‚ HIGH STEADY  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚                â”‚
        â–¼              â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚peak_raw_data â”‚  â”‚peak_raw_   â”‚  â”‚error_patterns   â”‚
â”‚(replaced val)â”‚  â”‚data (real) â”‚  â”‚(vÅ¡echny errory) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                   â”‚
       â–¼                â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           update_aggregation.py                   â”‚
â”‚         (refresh baseline kaÅ¾dÃ½ch 15min)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼               â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚peak_         â”‚  â”‚known_peaks  â”‚  â”‚known_issues  â”‚
â”‚investigation â”‚  â”‚(expected)   â”‚  â”‚(bugs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                 â”‚
                â–¼                 â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    match_known_issues.py        â”‚
       â”‚    match_known_peaks.py         â”‚
       â”‚  (pattern matching + context)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     ai_analyze_peaks.py         â”‚
       â”‚  (AI inference - OpenAI/Claude) â”‚
       â”‚   + analyze_steady_errors.py    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼               â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Teams alert  â”‚  â”‚ Auto-ticket â”‚  â”‚ Auto-fix PR  â”‚
â”‚ (critical)   â”‚  â”‚ (trends)    â”‚  â”‚ (GHC)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜




ðŸ“‹ COMPLETE WORKFLOW CHAIN
INIT FÃ¡ze âœ… (HOTOVO)
Peak_raw_data: 10,861 Å™Ã¡dkÅ¯ (21 dnÅ¯ reÃ¡lnÃ½ch dat)
Aggregation_data: 8,064 Å™Ã¡dkÅ¯ (kompletnÃ­ tÃ½dennÃ­ grid)
REGULAR FÃ¡ze (dennÃ­)
1) Data collection (collect_peak_detailed.py)

SbÃ­rÃ¡ data z ES kaÅ¾dÃ½ch 15 min
Output: DATA|timestamp|day|hour|quarter|namespace|mean|stddev|samples
2) Peak detection + DB write (ingest_from_log_v2.py BEZ --init flag)

PorovnÃ¡nÃ­:
Aggregation baseline (z aggregation_data)
3 pÅ™edchozÃ­ okna stejnÃ½ den (z peak_raw_data)
Decision: ratio â‰¥ 15Ã— AND value â‰¥ 100 â†’ PEAK!
3a) NO PEAK:

â†’ INSERT do peak_raw_data
3b) PEAK detected:

â†’ INSERT do peak_investigation (full context)
â†’ INSERT do peak_raw_data s nahrazenou hodnotou (reference mÃ­sto peaku)
4) Update aggregation (update_aggregation.py - CHYBÃ, musÃ­m vytvoÅ™it)

Po kaÅ¾dÃ©m insertu do peak_raw_data
PÅ™epoÄÃ­tÃ¡ mean/stddev/samples v aggregation_data
Rolling update (zachovÃ¡ 7dennÃ­ okno)
5) Known issues matching (match_known_issues.py - CHYBÃ)

Po detekci peaku
PorovnÃ¡ error pattern s known_issues tabulkou
Pokud match â†’ aktualizuje peak_investigation.known_issue_id
6) AI Analysis (ai_analyze_peaks.py - CHYBÃ, fÃ¡ze 5)

NaÄte novÃ© peaky z peak_investigation (WHERE ai_analysis IS NULL)
ZavolÃ¡ AI (OpenAI/Claude) s kontextem
UloÅ¾Ã­ analÃ½zu do peak_investigation.ai_analysis
7) Teams notification (send_teams_notification.py - ÄŒÃSTEÄŒNÄš EXISTUJE)

Po AI analÃ½ze
FormÃ¡tuje message s AI insights
PoÅ¡le do Teams kanÃ¡lu
8) Auto-fix PR (create_fix_pr.py - fÃ¡ze 5, GHC)

Pokud AI confidence > 0.8 AND je to znÃ¡mÃ½ pattern
GitHub Copilot vytvoÅ™Ã­ PR s fixem


Known Issues vs Known Peaks
MÃ¡Å¡ naprostou pravdu - jsou 2 kategorie:

A) known_peaks - OÄekÃ¡vanÃ© anomÃ¡lie
Scheduled jobs (dennÃ­ backup v 2:00 â†’ spike errorÅ¯)
Maintenance windows
ZnÃ¡mÃ© deploymenty
Nejsou problÃ©my, ale oÄekÃ¡vanÃ© vzorce
Akce: Suppress notifikaci, zalogovat jako "expected"
B) known_issues - ZnÃ¡mÃ© bugs/problÃ©my
OpakujÃ­cÃ­ se errory (OutOfMemory na service X)
ZnÃ¡mÃ© regression bugs
Timeout na endpoint Y
JSOU problÃ©my, ale uÅ¾ identifikovanÃ©
Akce: Match pattern, doplnit context, link na ticket/PR
**C) VÅ¡echny ostatnÃ­ errory (mimo peaky)
PotÅ™ebujeme tabulku error_patterns pro tracking vÅ¡ech errorÅ¯
AI analÃ½za i pro non-peak errory (napÅ™. steady 50 errors/15min = moÅ¾nÃ½ memory leak)
Tracking trendu (pomalu roste â†’ problÃ©m, ale ne peak)

Known Peaks - JIÅ½ VIDÄšNÃ‰/VYÅ˜EÅ ENÃ‰
SprÃ¡vnÄ›jÅ¡Ã­ popis:

known_peaks = Katalog vyÅ™eÅ¡enÃ½ch problÃ©mÅ¯
Peak kterÃ½ uÅ¾ byl, byl investigovÃ¡n, mÃ¡ znÃ¡mÃ© Å™eÅ¡enÃ­
Pattern matching: NovÃ½ peak â†’ match na known_peak â†’ instant context
Obsah:
Error pattern (stacktrace, message)
Root cause (co to zpÅ¯sobilo)
Solution (jak vyÅ™eÅ¡it)
Prevention (jak pÅ™edejÃ­t)
Link na ticket/PR
Benefit: PÅ™i matchi nenÃ­ tÅ™eba investigace, rovnou poÅ¡leme Å™eÅ¡enÃ­
PÅ™Ã­klad:
Known Peak #42:
  Pattern: "OutOfMemoryError in pcb-sit-01-app"
  Root Cause: Memory leak v UserService.getUsers()
  Solution: Restart pod + hotfix v2.3.2
  Prevention: Upgrade na v2.4.0 (fix je tam)
  Ticket: JIRA-1234
NovÃ½ peak â†’ match â†’ "Toto je znÃ¡mÃ½ problÃ©m JIRA-1234, Å™eÅ¡enÃ­: restart + upgrade"

ðŸ”§ 3) Evidence VÅ ECH errorÅ¯ - rychlejÅ¡Ã­ analÃ½za
NovÃ¡ tabulka: error_patterns
CREATE TABLE error_patterns (
  id SERIAL PRIMARY KEY,
  namespace VARCHAR(255),
  error_type VARCHAR(255),
  error_message TEXT,
  pattern_hash VARCHAR(64),  -- hash pro rychlÃ© matching
  
  -- Statistics
  first_seen TIMESTAMP,
  last_seen TIMESTAMP,
  occurrence_count INT,
  avg_errors_per_15min FLOAT,
  
  -- Classification
  severity VARCHAR(50),  -- low, medium, high, critical
  is_peak BOOLEAN,
  is_steady BOOLEAN,  -- steady high errors (ne peak, ale problÃ©m)
  
  -- Analysis
  ai_analysis TEXT,
  root_cause VARCHAR(255),
  solution TEXT,
  
  -- Links
  related_ticket VARCHAR(100),
  related_pr VARCHAR(100)
);

Tracking script: track_all_errors.py

SbÃ­rÃ¡ vÅ¡echny errory z ES (ne jen agregovanÃ© okna)
VytvÃ¡Å™Ã­ pattern hash (stacktrace + message)
UklÃ¡dÃ¡ do error_patterns
AI analÃ½za i pro non-peak errory


K8s Deployment (poznÃ¡mka k bodu 5):
CronJob kaÅ¾dÃ½ch 15 min:
schedule: "*/15 * * * *"
containers:
  - collect_peak_detailed.py (ES â†’ .txt)
  - ingest_from_log_v2.py (txt â†’ DB, peak detection)
  - update_aggregation.py (refresh baseline)
  - match_known_issues.py (pattern matching)
  - ai_analyze_peaks.py (AI inference)
  - send_teams_notification.py (alert)
  
  
ChybÄ›jÃ­cÃ­ scripty k implementaci:

update_aggregation.py - rolling update aggregation_data
match_known_issues.py - pattern matching
create_fix_pr.py - auto-fix (fÃ¡ze 5, GHC)
track_all_errors.py -new
- doladit peak detection


SPRÃVNÃ Algoritmus (z dokumentace):
To, co jsem Å™Ã­kal, byla TOTÃLNÃ PITOMOST!
# REFERENCE 1: BASELINE (1 tÃ½den - aggregation_data)
baseline_mean = aggregation[day, hour, quarter, namespace].mean

# REFERENCE 2: SAME DAY (poslednÃ­ 4 okna dnes z peak_raw_data)
same_day_windows = [poslednÃ­ 4 okna z raw_data dnes]
same_day_mean = average(same_day_windows)

# FINAL REFERENCE
final_ref = average([baseline_mean, same_day_mean])

# PEAK DECISION
is_peak = (ratio >= 15?) AND..?


ðŸ”‘ KlÃ­ÄovÃ© rozdÃ­ly:
âœ… Layer 2 SPRÃVNÄš: StejnÃ© okno (10:30) ze STEJNÃ‰HO dne TÃDNE v minulÃ½ch tÃ½dnech â†’ To je baseline_mean v aggregation_data!

âœ… NahrazovÃ¡nÃ­:

Detekujeme peak â†’ zaznamenÃ¡me ORIGINÃLNÃ hodnotu (5000) do peak_investigation
Do peak_raw_data uloÅ¾Ã­me REFERENÄŒNÃ hodnotu (replacement)
DalÅ¡Ã­ okna pak budou pouÅ¾Ã­vat Äistou referenci
âœ… Ratio >= 15 AND value >= 100 - To je DYNAMICKÃ‰ na baseline, ne hardcoded!

REFERENCE 2: SAME DAY (poslednÃ­ 4 okna dnes z peak_raw_data) - dame 3, ale sbirat budeme 4, pro jistotu, ono stejne to ctvrte je to, ktere se aktualne sbira.... s tim se to neda porovnat
PEAK DECISION
is_peak = (ratio >= 15) ... pouze, zadna absolutni hodnota jako (value >= 100), spis by se tam namisto 100 dal pouzit treba celkovy prumer za poslednich 24h pro dane NS a jeste je otazka jestli ratio ma byt taky hardcodovane, jestli by nemelo byt dynamicke, 15 se mi zda moc a dost absolutni